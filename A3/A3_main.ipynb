{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea9e5d9-937b-40b6-a0e2-0669880dc28d",
   "metadata": {},
   "source": [
    "# Advanced NLP Assignment 3, Group 1\n",
    "Thijs Vollebregt(2670637), Chuqiao Guo(2798305), Yijing Zhang(2818171), Danna Shao(2663369)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6d7f2-e307-4ce8-ac3b-037d11703020",
   "metadata": {},
   "source": [
    "### Importing libraries and utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bab441c-e114-4778-884c-a7d74b602788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "from read_and_preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc19b5-066f-4534-86e4-97238b53cc73",
   "metadata": {},
   "source": [
    "### Basic settings\n",
    "- We use seqeval as the metric, and bert-base-uncased as BERT model with its corresponding auto tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfb19ea-7a4f-488a-800e-34ccb0532a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "task = \"srl\"\n",
    "model_checkpoint = \"bert-base-uncased\" # bert-base-uncased for better percision, distilbert-base-uncased for faster run\n",
    "batch_size = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15ca83-82fa-4ab5-a709-0873f9000ad0",
   "metadata": {},
   "source": [
    "## 1. Baseline model\n",
    "The appraoch used for the baseline model is basically converting the sentence into the following form:\n",
    "\n",
    "> [CLS] This is the sentence content [SEP] is [SEP].\n",
    "\n",
    "And this is realized by simply using the logic of the auto tokenizer:\n",
    "`tokenizer(list1,list2)` will return [CLS] list1 content [SEP] list2 content [SEP]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d78048-52bb-4602-a41d-66151566d50f",
   "metadata": {},
   "source": [
    "### 1.1 Importing datasets and libraries:\n",
    "Corpus is load and preprocessed into huggingface dataset type by baseline_ds.py. This script contains following functions and variables for the baseline model:\n",
    "\n",
    "Dataset preparation:\n",
    "- `get_mappings_dict()`: Get the dictionary mapping string classes (e.g. 'ARG0') to int labels and its reverse.\n",
    "- `create_word_sentlist()`: Create dictionary containing the required data for generating desired huggingface dataset.\n",
    "- `tokenize_and_align_labels()`: Adapted from the example notebook. Solves label alignment after re-tokenization.\n",
    "    \n",
    "Evaluation:\n",
    "- `compute_metrics()`: Compute the overall percision, recall and f1.\n",
    "- `reverse_label()`: Map the int class labels back to strings.\n",
    "- `class_results()`: Compute the percision, recall and f1 for each class.\n",
    "\n",
    "Variables: \n",
    "- `label_dict, label_dict_rev`: The mapping dictionary from string class to int class and its reverse.\n",
    "- `tokenized_train, tokenized_dev, tokenized_test`: The tokenized and ready-to-use datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671f05a0-3c19-41b9-a45b-44ca37e0f4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167165f2044d438d81799d3a9789843b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a88025788b4dfd9fa721247995a0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f9011214564703969e31d15c4909bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import baseline_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05768c52-bd05-42f2-9c85-f564b171ebdb",
   "metadata": {},
   "source": [
    "### 1.2 Creating basline model:\n",
    "The model is trained on the tokenized trian dataset, and evaluated on the dev dataset in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ae8d42-e091-42c8-8ffe-c6d5460adcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(baseline_ds.label_dict))\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "label_list = baseline_ds.label_list\n",
    "\n",
    "base_trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=baseline_ds.tokenized_train,\n",
    "    eval_dataset=baseline_ds.tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=baseline_ds.compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa887ac8-d6d4-498d-a1d0-ddf1a3c3f2b8",
   "metadata": {},
   "source": [
    "### 1.3 Train and evaluate:\n",
    "The evaluation is done on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfe3640-e5fa-4e08-8084-03e8a4272305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7965' max='7965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7965/7965 05:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>0.798406</td>\n",
       "      <td>0.819914</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>0.970571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.105916</td>\n",
       "      <td>0.817995</td>\n",
       "      <td>0.827277</td>\n",
       "      <td>0.822610</td>\n",
       "      <td>0.972739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.109497</td>\n",
       "      <td>0.820079</td>\n",
       "      <td>0.833860</td>\n",
       "      <td>0.826912</td>\n",
       "      <td>0.973297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7965, training_loss=0.07123838856919187, metrics={'train_runtime': 352.3275, 'train_samples_per_second': 361.59, 'train_steps_per_second': 22.607, 'total_flos': 4555060725912480.0, 'train_loss': 0.07123838856919187, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adcce3-f043-4a19-8109-5754f28b059c",
   "metadata": {},
   "source": [
    "- Getting the prediction and true lables for the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3c210b-4dee-41f0-bff3-dc6a014dc3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_pred, base_labels, _ = base_trainer.predict(baseline_ds.tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64dca2-d79d-40b8-be20-8fccd5883163",
   "metadata": {},
   "source": [
    "- Evaluate the results of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217d9c29-0f27-43ec-bf2f-111844530413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': {'precision': 0.7312252964426877,\n",
       "  'recall': 0.7142857142857143,\n",
       "  'f1': 0.72265625,\n",
       "  'number': 259},\n",
       " 'ADV': {'precision': 0.6910569105691057,\n",
       "  'recall': 0.6378986866791745,\n",
       "  'f1': 0.6634146341463414,\n",
       "  'number': 533},\n",
       " 'ARG0': {'precision': 0.9242424242424242,\n",
       "  'recall': 0.8714285714285714,\n",
       "  'f1': 0.8970588235294117,\n",
       "  'number': 70},\n",
       " 'ARG1': {'precision': 0.6554621848739496,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.6995515695067266,\n",
       "  'number': 104},\n",
       " 'ARG1-DSP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARG2': {'precision': 0.2,\n",
       "  'recall': 0.25,\n",
       "  'f1': 0.22222222222222224,\n",
       "  'number': 8},\n",
       " 'ARG3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'ARGM-ADJ': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-ADV': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-CXN': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.4,\n",
       "  'f1': 0.3636363636363636,\n",
       "  'number': 5},\n",
       " 'ARGM-DIR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-LOC': {'precision': 0.6666666666666666,\n",
       "  'recall': 0.8,\n",
       "  'f1': 0.7272727272727272,\n",
       "  'number': 10},\n",
       " 'ARGM-MNR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 8},\n",
       " 'ARGM-TMP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'CAU': {'precision': 0.5666666666666667,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1': 0.6126126126126126,\n",
       "  'number': 51},\n",
       " 'COM': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.38461538461538464,\n",
       "  'f1': 0.3571428571428571,\n",
       "  'number': 13},\n",
       " 'CXN': {'precision': 0.6428571428571429,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.6923076923076924,\n",
       "  'number': 12},\n",
       " 'DIR': {'precision': 0.47619047619047616,\n",
       "  'recall': 0.425531914893617,\n",
       "  'f1': 0.449438202247191,\n",
       "  'number': 47},\n",
       " 'DIS': {'precision': 0.7873015873015873,\n",
       "  'recall': 0.8294314381270903,\n",
       "  'f1': 0.8078175895765471,\n",
       "  'number': 299},\n",
       " 'DSP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'EXT': {'precision': 0.780952380952381,\n",
       "  'recall': 0.7454545454545455,\n",
       "  'f1': 0.7627906976744186,\n",
       "  'number': 110},\n",
       " 'GOL': {'precision': 0.5,\n",
       "  'recall': 0.20833333333333334,\n",
       "  'f1': 0.29411764705882354,\n",
       "  'number': 24},\n",
       " 'LOC': {'precision': 0.5756302521008403,\n",
       "  'recall': 0.6555023923444976,\n",
       "  'f1': 0.6129753914988815,\n",
       "  'number': 209},\n",
       " 'LVB': {'precision': 0.725,\n",
       "  'recall': 0.8169014084507042,\n",
       "  'f1': 0.7682119205298014,\n",
       "  'number': 71},\n",
       " 'MNR': {'precision': 0.5855263157894737,\n",
       "  'recall': 0.5705128205128205,\n",
       "  'f1': 0.577922077922078,\n",
       "  'number': 156},\n",
       " 'MOD': {'precision': 0.9326530612244898,\n",
       "  'recall': 0.970276008492569,\n",
       "  'f1': 0.9510926118626432,\n",
       "  'number': 471},\n",
       " 'NEG': {'precision': 0.9177489177489178,\n",
       "  'recall': 0.9592760180995475,\n",
       "  'f1': 0.9380530973451326,\n",
       "  'number': 221},\n",
       " 'PRD': {'precision': 0.45161290322580644,\n",
       "  'recall': 0.3181818181818182,\n",
       "  'f1': 0.3733333333333333,\n",
       "  'number': 44},\n",
       " 'PRP': {'precision': 0.5581395348837209,\n",
       "  'recall': 0.631578947368421,\n",
       "  'f1': 0.5925925925925927,\n",
       "  'number': 76},\n",
       " 'PRR': {'precision': 0.7878787878787878,\n",
       "  'recall': 0.7536231884057971,\n",
       "  'f1': 0.7703703703703704,\n",
       "  'number': 69},\n",
       " 'RG0': {'precision': 0.8710100568430258,\n",
       "  'recall': 0.907103825136612,\n",
       "  'f1': 0.8886906089672094,\n",
       "  'number': 2196},\n",
       " 'RG1': {'precision': 0.8440088348978465,\n",
       "  'recall': 0.885060799073538,\n",
       "  'f1': 0.8640474844544941,\n",
       "  'number': 3454},\n",
       " 'RG2': {'precision': 0.7552565180824222,\n",
       "  'recall': 0.7835951134380453,\n",
       "  'f1': 0.7691648822269808,\n",
       "  'number': 1146},\n",
       " 'RG3': {'precision': 0.7142857142857143,\n",
       "  'recall': 0.33783783783783783,\n",
       "  'f1': 0.4587155963302752,\n",
       "  'number': 74},\n",
       " 'RG4': {'precision': 0.640625,\n",
       "  'recall': 0.7321428571428571,\n",
       "  'f1': 0.6833333333333332,\n",
       "  'number': 56},\n",
       " 'RG5': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'RGA': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'TMP': {'precision': 0.7986577181208053,\n",
       "  'recall': 0.8576576576576577,\n",
       "  'f1': 0.8271068635968724,\n",
       "  'number': 555},\n",
       " 'V': {'precision': 0.7647058823529411,\n",
       "  'recall': 0.8125,\n",
       "  'f1': 0.787878787878788,\n",
       "  'number': 16},\n",
       " '_': {'precision': 0.7772353851649005,\n",
       "  'recall': 0.798629801810619,\n",
       "  'f1': 0.7877873649912508,\n",
       "  'number': 8174},\n",
       " 'overall_precision': 0.7944717830593655,\n",
       " 'overall_recall': 0.8179368330279185,\n",
       " 'overall_f1': 0.8060335670278309,\n",
       " 'overall_accuracy': 0.974109952388013}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_ds.class_results(base_pred, base_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf87ea-280f-41e7-83c1-f4a635c16b43",
   "metadata": {},
   "source": [
    "### 1.4 Limitations of the baseline model:\n",
    "- Cannot precisely handle sentences with duplicated predicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527f872-b91c-46f4-981e-eda79d3576f5",
   "metadata": {},
   "source": [
    "## 2. More advanced model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b89d83-0bc8-4d9c-a63c-b25d4cfb630a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
