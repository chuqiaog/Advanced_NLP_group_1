{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea9e5d9-937b-40b6-a0e2-0669880dc28d",
   "metadata": {},
   "source": [
    "# Advanced NLP Assignment 3, Group 1\n",
    "Thijs Vollebregt(2670637), Chuqiao Guo(2798305), Yijing Zhang(2818171), Danna Shao(2663369)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6d7f2-e307-4ce8-ac3b-037d11703020",
   "metadata": {},
   "source": [
    "### Importing libraries and utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bab441c-e114-4778-884c-a7d74b602788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "from read_and_preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc19b5-066f-4534-86e4-97238b53cc73",
   "metadata": {},
   "source": [
    "### Basic settings\n",
    "- We use seqeval as the metric, and bert-base-uncased as BERT model with its corresponding auto tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfb19ea-7a4f-488a-800e-34ccb0532a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "task = \"srl\"\n",
    "model_checkpoint = \"bert-base-uncased\" # bert-base-uncased for better percision, distilbert-base-uncased for faster run\n",
    "batch_size = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15ca83-82fa-4ab5-a709-0873f9000ad0",
   "metadata": {},
   "source": [
    "## 1. Baseline model\n",
    "The appraoch used for the baseline model is basically converting the sentence into the following form:\n",
    "\n",
    "> [CLS] This is the sentence content [SEP] is [SEP].\n",
    "\n",
    "And this is realized by simply using the logic of the auto tokenizer:\n",
    "`tokenizer(list1,list2)` will return [CLS] list1 content [SEP] list2 content [SEP]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d78048-52bb-4602-a41d-66151566d50f",
   "metadata": {},
   "source": [
    "### 1.1 Importing datasets and libraries:\n",
    "Corpus is load and preprocessed into huggingface dataset type by baseline_ds.py. This script contains following functions and variables for the baseline model:\n",
    "\n",
    "Dataset preparation:\n",
    "- `get_mappings_dict()`: Get the dictionary mapping string classes (e.g. 'ARG0') to int labels and its reverse.\n",
    "- `create_word_sentlist()`: Create dictionary containing the required data for generating desired huggingface dataset.\n",
    "- `tokenize_and_align_labels()`: Adapted from the example notebook. Solves label alignment after re-tokenization.\n",
    "    \n",
    "Evaluation:\n",
    "- `compute_metrics()`: Compute the overall percision, recall and f1.\n",
    "- `reverse_label()`: Map the int class labels back to strings.\n",
    "- `class_results()`: Compute the percision, recall and f1 for each class.\n",
    "\n",
    "Variables: \n",
    "- `label_dict, label_dict_rev`: The mapping dictionary from string class to int class and its reverse.\n",
    "- `tokenized_train, tokenized_dev, tokenized_test`: The tokenized and ready-to-use datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671f05a0-3c19-41b9-a45b-44ca37e0f4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9484aad61d24b1e8ccd6c6c919ca50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40626e85994e41f79ed0164367ccb1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e756458b9448daac888a5cce3a4f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import baseline_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e294f-911e-4979-992c-2dd5fbc7f6ee",
   "metadata": {},
   "source": [
    "Inspect the tokenized (ready to use) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5eefd0-9e83-4c3f-9fce-f037df981e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['What', 'if', 'Google', 'Morphed', 'Into', 'GoogleOS', '?'], 'srl_arg_tags': [29, 29, 11, 29, 29, 43, 29], 'pred': ['Morphed'], 'input_ids': [101, 2054, 2065, 8224, 22822, 8458, 2098, 2046, 8224, 2891, 1029, 102, 22822, 8458, 2098, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 3, 3, 3, 4, 5, 5, 6, None, 0, 0, 0, None], 'labels': [-100, 29, 29, 11, 29, 29, 29, 29, 43, 43, 29, -100, 29, 29, 29, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(baseline_ds.tokenized_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896ea36-2523-45ae-9dcf-5e741b82c721",
   "metadata": {},
   "source": [
    "We can see that the last 4 input ids are 102, x, x, 102 where 102 is the labels of [SEP], and their corresponding word_ids are None, indicating the tokenization worked properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05768c52-bd05-42f2-9c85-f564b171ebdb",
   "metadata": {},
   "source": [
    "### 1.2 Creating basline model:\n",
    "The model is trained on the tokenized trian dataset, and evaluated on the dev dataset in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ae8d42-e091-42c8-8ffe-c6d5460adcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(baseline_ds.label_dict))\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "label_list = baseline_ds.label_list\n",
    "\n",
    "base_trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=baseline_ds.tokenized_train,\n",
    "    eval_dataset=baseline_ds.tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=baseline_ds.compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa887ac8-d6d4-498d-a1d0-ddf1a3c3f2b8",
   "metadata": {},
   "source": [
    "### 1.3 Train and evaluate:\n",
    "The evaluation is done on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfe3640-e5fa-4e08-8084-03e8a4272305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7965' max='7965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7965/7965 05:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>0.798406</td>\n",
       "      <td>0.819914</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>0.970571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.105916</td>\n",
       "      <td>0.817995</td>\n",
       "      <td>0.827277</td>\n",
       "      <td>0.822610</td>\n",
       "      <td>0.972739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.109497</td>\n",
       "      <td>0.820079</td>\n",
       "      <td>0.833860</td>\n",
       "      <td>0.826912</td>\n",
       "      <td>0.973297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7965, training_loss=0.07123838856919187, metrics={'train_runtime': 352.3275, 'train_samples_per_second': 361.59, 'train_steps_per_second': 22.607, 'total_flos': 4555060725912480.0, 'train_loss': 0.07123838856919187, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51adcce3-f043-4a19-8109-5754f28b059c",
   "metadata": {},
   "source": [
    "- Getting the prediction and true lables for the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3c210b-4dee-41f0-bff3-dc6a014dc3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_pred, base_labels, _ = base_trainer.predict(baseline_ds.tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64dca2-d79d-40b8-be20-8fccd5883163",
   "metadata": {},
   "source": [
    "- Evaluate the results of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217d9c29-0f27-43ec-bf2f-111844530413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': {'precision': 0.7312252964426877,\n",
       "  'recall': 0.7142857142857143,\n",
       "  'f1': 0.72265625,\n",
       "  'number': 259},\n",
       " 'ADV': {'precision': 0.6910569105691057,\n",
       "  'recall': 0.6378986866791745,\n",
       "  'f1': 0.6634146341463414,\n",
       "  'number': 533},\n",
       " 'ARG0': {'precision': 0.9242424242424242,\n",
       "  'recall': 0.8714285714285714,\n",
       "  'f1': 0.8970588235294117,\n",
       "  'number': 70},\n",
       " 'ARG1': {'precision': 0.6554621848739496,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.6995515695067266,\n",
       "  'number': 104},\n",
       " 'ARG1-DSP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARG2': {'precision': 0.2,\n",
       "  'recall': 0.25,\n",
       "  'f1': 0.22222222222222224,\n",
       "  'number': 8},\n",
       " 'ARG3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'ARGM-ADJ': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-ADV': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-CXN': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.4,\n",
       "  'f1': 0.3636363636363636,\n",
       "  'number': 5},\n",
       " 'ARGM-DIR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-LOC': {'precision': 0.6666666666666666,\n",
       "  'recall': 0.8,\n",
       "  'f1': 0.7272727272727272,\n",
       "  'number': 10},\n",
       " 'ARGM-MNR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 8},\n",
       " 'ARGM-TMP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'CAU': {'precision': 0.5666666666666667,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1': 0.6126126126126126,\n",
       "  'number': 51},\n",
       " 'COM': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.38461538461538464,\n",
       "  'f1': 0.3571428571428571,\n",
       "  'number': 13},\n",
       " 'CXN': {'precision': 0.6428571428571429,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.6923076923076924,\n",
       "  'number': 12},\n",
       " 'DIR': {'precision': 0.47619047619047616,\n",
       "  'recall': 0.425531914893617,\n",
       "  'f1': 0.449438202247191,\n",
       "  'number': 47},\n",
       " 'DIS': {'precision': 0.7873015873015873,\n",
       "  'recall': 0.8294314381270903,\n",
       "  'f1': 0.8078175895765471,\n",
       "  'number': 299},\n",
       " 'DSP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'EXT': {'precision': 0.780952380952381,\n",
       "  'recall': 0.7454545454545455,\n",
       "  'f1': 0.7627906976744186,\n",
       "  'number': 110},\n",
       " 'GOL': {'precision': 0.5,\n",
       "  'recall': 0.20833333333333334,\n",
       "  'f1': 0.29411764705882354,\n",
       "  'number': 24},\n",
       " 'LOC': {'precision': 0.5756302521008403,\n",
       "  'recall': 0.6555023923444976,\n",
       "  'f1': 0.6129753914988815,\n",
       "  'number': 209},\n",
       " 'LVB': {'precision': 0.725,\n",
       "  'recall': 0.8169014084507042,\n",
       "  'f1': 0.7682119205298014,\n",
       "  'number': 71},\n",
       " 'MNR': {'precision': 0.5855263157894737,\n",
       "  'recall': 0.5705128205128205,\n",
       "  'f1': 0.577922077922078,\n",
       "  'number': 156},\n",
       " 'MOD': {'precision': 0.9326530612244898,\n",
       "  'recall': 0.970276008492569,\n",
       "  'f1': 0.9510926118626432,\n",
       "  'number': 471},\n",
       " 'NEG': {'precision': 0.9177489177489178,\n",
       "  'recall': 0.9592760180995475,\n",
       "  'f1': 0.9380530973451326,\n",
       "  'number': 221},\n",
       " 'PRD': {'precision': 0.45161290322580644,\n",
       "  'recall': 0.3181818181818182,\n",
       "  'f1': 0.3733333333333333,\n",
       "  'number': 44},\n",
       " 'PRP': {'precision': 0.5581395348837209,\n",
       "  'recall': 0.631578947368421,\n",
       "  'f1': 0.5925925925925927,\n",
       "  'number': 76},\n",
       " 'PRR': {'precision': 0.7878787878787878,\n",
       "  'recall': 0.7536231884057971,\n",
       "  'f1': 0.7703703703703704,\n",
       "  'number': 69},\n",
       " 'RG0': {'precision': 0.8710100568430258,\n",
       "  'recall': 0.907103825136612,\n",
       "  'f1': 0.8886906089672094,\n",
       "  'number': 2196},\n",
       " 'RG1': {'precision': 0.8440088348978465,\n",
       "  'recall': 0.885060799073538,\n",
       "  'f1': 0.8640474844544941,\n",
       "  'number': 3454},\n",
       " 'RG2': {'precision': 0.7552565180824222,\n",
       "  'recall': 0.7835951134380453,\n",
       "  'f1': 0.7691648822269808,\n",
       "  'number': 1146},\n",
       " 'RG3': {'precision': 0.7142857142857143,\n",
       "  'recall': 0.33783783783783783,\n",
       "  'f1': 0.4587155963302752,\n",
       "  'number': 74},\n",
       " 'RG4': {'precision': 0.640625,\n",
       "  'recall': 0.7321428571428571,\n",
       "  'f1': 0.6833333333333332,\n",
       "  'number': 56},\n",
       " 'RG5': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'RGA': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'TMP': {'precision': 0.7986577181208053,\n",
       "  'recall': 0.8576576576576577,\n",
       "  'f1': 0.8271068635968724,\n",
       "  'number': 555},\n",
       " 'V': {'precision': 0.7647058823529411,\n",
       "  'recall': 0.8125,\n",
       "  'f1': 0.787878787878788,\n",
       "  'number': 16},\n",
       " '_': {'precision': 0.7772353851649005,\n",
       "  'recall': 0.798629801810619,\n",
       "  'f1': 0.7877873649912508,\n",
       "  'number': 8174},\n",
       " 'overall_precision': 0.7944717830593655,\n",
       " 'overall_recall': 0.8179368330279185,\n",
       " 'overall_f1': 0.8060335670278309,\n",
       " 'overall_accuracy': 0.974109952388013}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_ds.class_results(base_pred, base_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf87ea-280f-41e7-83c1-f4a635c16b43",
   "metadata": {},
   "source": [
    "### 1.4 Limitations of the baseline model:\n",
    "- Cannot precisely handle sentences with duplicated predicates.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527f872-b91c-46f4-981e-eda79d3576f5",
   "metadata": {},
   "source": [
    "## 2. More advanced model:\n",
    "The more advanced model uses similar apporach as the Augment method described in [NegBERT (Khandelwal, et al. 2020)](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.704.pdf). That is, adding a special token ([V]) immediately before the predicate:\n",
    "> This [V] is a sentence.\n",
    "\n",
    "(Note that **the special token and the predicate is considered a whole**. That is, the actual sentence is like\n",
    "> 'This' **'[V] is'** 'a' 'sentence' '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f128f1-28e3-4a39-85fb-7320f574ae4c",
   "metadata": {},
   "source": [
    "### 2.1 Implementation\n",
    "The implementation is actually simpler than the basic model, and the coding are only slightly adjusted.\n",
    "\n",
    "We now use `augment_sentlist()` to create a list of sentences that having all the predicates augmented into '[V] pred', and removed the [SEP] part from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed55d9a3-b67f-4699-8260-9018247a493e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfa485c2eb840ccaa7c3f81a83cfc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc8a6ca53d146ee9cd4bd72dcb8e66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5441 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f449dc6bd30487390b9d3833b566d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import advanced_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a985e05-67e6-4943-bbcc-b8299efa8127",
   "metadata": {},
   "source": [
    "Inspect the tokenized data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15723ffa-75cd-4a54-8a24-2dc2c29d4c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['What', 'if', 'Google', '[V] Morphed', 'Into', 'GoogleOS', '?'], 'srl_arg_tags': [29, 29, 11, 29, 29, 43, 29], 'input_ids': [101, 2054, 2065, 8224, 1031, 1058, 1033, 22822, 8458, 2098, 2046, 8224, 2891, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 3, 3, 3, 3, 3, 3, 4, 5, 5, 6, None], 'labels': [-100, 29, 29, 11, 29, 29, 29, 29, 29, 29, 29, 43, 43, 29, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(advanced_ds.tokenized_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94007f01-960d-4707-bda0-22b3a1b15d66",
   "metadata": {},
   "source": [
    "The predicate is correctly augmented into '[V] pred'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1afdd-4b3b-419e-bbdf-d78b1ef1d278",
   "metadata": {},
   "source": [
    "### 2.2 Creating the advanced model, train and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e72d632f-a075-4da9-a62d-f5b48ecd9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=advanced_ds.tokenized_train,\n",
    "    eval_dataset=advanced_ds.tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=baseline_ds.compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c59de392-6ad1-48dd-891e-9d26d39e996f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7965' max='7965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7965/7965 05:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.086001</td>\n",
       "      <td>0.851091</td>\n",
       "      <td>0.861808</td>\n",
       "      <td>0.856416</td>\n",
       "      <td>0.980564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.089585</td>\n",
       "      <td>0.855718</td>\n",
       "      <td>0.866338</td>\n",
       "      <td>0.860995</td>\n",
       "      <td>0.981127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.094870</td>\n",
       "      <td>0.860633</td>\n",
       "      <td>0.866519</td>\n",
       "      <td>0.863566</td>\n",
       "      <td>0.981430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7965, training_loss=0.03031403552939588, metrics={'train_runtime': 339.2309, 'train_samples_per_second': 375.549, 'train_steps_per_second': 23.48, 'total_flos': 4617691475641200.0, 'train_loss': 0.03031403552939588, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f95db0-cf51-4d89-8b28-88f4fb3e0dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "advanced_pred, advanced_labels, _ = advanced_trainer.predict(advanced_ds.tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588162c-2859-4537-9772-1aa30a158a70",
   "metadata": {},
   "source": [
    "- Evaluate the results of each class (slightly better than the baseline model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02664dbc-fa47-41e7-8a64-11622cf14d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': {'precision': 0.7361111111111112,\n",
       "  'recall': 0.7130044843049327,\n",
       "  'f1': 0.7243735763097949,\n",
       "  'number': 223},\n",
       " 'ADV': {'precision': 0.7036247334754797,\n",
       "  'recall': 0.6707317073170732,\n",
       "  'f1': 0.6867845993756504,\n",
       "  'number': 492},\n",
       " 'ARG0': {'precision': 0.9393939393939394,\n",
       "  'recall': 0.8857142857142857,\n",
       "  'f1': 0.9117647058823529,\n",
       "  'number': 70},\n",
       " 'ARG1': {'precision': 0.7788461538461539,\n",
       "  'recall': 0.7788461538461539,\n",
       "  'f1': 0.7788461538461539,\n",
       "  'number': 104},\n",
       " 'ARG1-DSP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARG2': {'precision': 0.23529411764705882,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.31999999999999995,\n",
       "  'number': 8},\n",
       " 'ARG3': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'ARGM-ADJ': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-ADV': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-CXN': {'precision': 0.5714285714285714,\n",
       "  'recall': 0.8,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 5},\n",
       " 'ARGM-DIR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'ARGM-LOC': {'precision': 1.0,\n",
       "  'recall': 0.7,\n",
       "  'f1': 0.8235294117647058,\n",
       "  'number': 10},\n",
       " 'ARGM-MNR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 8},\n",
       " 'ARGM-TMP': {'precision': 0.6666666666666666,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.8,\n",
       "  'number': 2},\n",
       " 'CAU': {'precision': 0.5666666666666667,\n",
       "  'recall': 0.7391304347826086,\n",
       "  'f1': 0.6415094339622641,\n",
       "  'number': 46},\n",
       " 'COM': {'precision': 0.38095238095238093,\n",
       "  'recall': 0.6153846153846154,\n",
       "  'f1': 0.47058823529411764,\n",
       "  'number': 13},\n",
       " 'CXN': {'precision': 0.5789473684210527,\n",
       "  'recall': 0.9166666666666666,\n",
       "  'f1': 0.7096774193548387,\n",
       "  'number': 12},\n",
       " 'DIR': {'precision': 0.45098039215686275,\n",
       "  'recall': 0.48936170212765956,\n",
       "  'f1': 0.46938775510204084,\n",
       "  'number': 47},\n",
       " 'DIS': {'precision': 0.7771739130434783,\n",
       "  'recall': 0.7857142857142857,\n",
       "  'f1': 0.7814207650273224,\n",
       "  'number': 182},\n",
       " 'DSP': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'EXT': {'precision': 0.7757009345794392,\n",
       "  'recall': 0.7980769230769231,\n",
       "  'f1': 0.7867298578199052,\n",
       "  'number': 104},\n",
       " 'GOL': {'precision': 0.46153846153846156,\n",
       "  'recall': 0.25,\n",
       "  'f1': 0.32432432432432434,\n",
       "  'number': 24},\n",
       " 'LOC': {'precision': 0.6591928251121076,\n",
       "  'recall': 0.7101449275362319,\n",
       "  'f1': 0.6837209302325582,\n",
       "  'number': 207},\n",
       " 'LVB': {'precision': 0.7948717948717948,\n",
       "  'recall': 0.8985507246376812,\n",
       "  'f1': 0.8435374149659864,\n",
       "  'number': 69},\n",
       " 'MNR': {'precision': 0.5694444444444444,\n",
       "  'recall': 0.5578231292517006,\n",
       "  'f1': 0.563573883161512,\n",
       "  'number': 147},\n",
       " 'MOD': {'precision': 0.9819004524886877,\n",
       "  'recall': 0.9819004524886877,\n",
       "  'f1': 0.9819004524886877,\n",
       "  'number': 442},\n",
       " 'NEG': {'precision': 0.9812206572769953,\n",
       "  'recall': 0.9675925925925926,\n",
       "  'f1': 0.9743589743589743,\n",
       "  'number': 216},\n",
       " 'PRD': {'precision': 0.36585365853658536,\n",
       "  'recall': 0.3409090909090909,\n",
       "  'f1': 0.3529411764705882,\n",
       "  'number': 44},\n",
       " 'PRP': {'precision': 0.5974025974025974,\n",
       "  'recall': 0.6133333333333333,\n",
       "  'f1': 0.6052631578947367,\n",
       "  'number': 75},\n",
       " 'PRR': {'precision': 0.7761194029850746,\n",
       "  'recall': 0.7536231884057971,\n",
       "  'f1': 0.7647058823529411,\n",
       "  'number': 69},\n",
       " 'RG0': {'precision': 0.8875772906127037,\n",
       "  'recall': 0.9164248403946604,\n",
       "  'f1': 0.9017704169046259,\n",
       "  'number': 1723},\n",
       " 'RG1': {'precision': 0.8864257515942909,\n",
       "  'recall': 0.9127579737335835,\n",
       "  'f1': 0.8993991680788784,\n",
       "  'number': 3198},\n",
       " 'RG2': {'precision': 0.8537693006357856,\n",
       "  'recall': 0.8355555555555556,\n",
       "  'f1': 0.844564240790656,\n",
       "  'number': 1125},\n",
       " 'RG3': {'precision': 0.6607142857142857,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.5692307692307693,\n",
       "  'number': 74},\n",
       " 'RG4': {'precision': 0.6557377049180327,\n",
       "  'recall': 0.7142857142857143,\n",
       "  'f1': 0.6837606837606838,\n",
       "  'number': 56},\n",
       " 'RG5': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'RGA': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'TMP': {'precision': 0.8647166361974405,\n",
       "  'recall': 0.8743068391866913,\n",
       "  'f1': 0.8694852941176471,\n",
       "  'number': 541},\n",
       " 'V': {'precision': 0.7222222222222222,\n",
       "  'recall': 0.8125,\n",
       "  'f1': 0.7647058823529411,\n",
       "  'number': 16},\n",
       " '_': {'precision': 0.8340905961723806,\n",
       "  'recall': 0.8432628062360802,\n",
       "  'f1': 0.8386516231743615,\n",
       "  'number': 7184},\n",
       " 'overall_precision': 0.8394317435683161,\n",
       " 'overall_recall': 0.8499335186752085,\n",
       " 'overall_f1': 0.8446499894891737,\n",
       " 'overall_accuracy': 0.9827437115942361}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_ds.class_results(advanced_pred, advanced_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42ef5c-528f-4156-8155-cc698aff77b0",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Descriptions and explanations (50%)\n",
    "- [Done] Description of both models and their limitations (in particular with respect to the input representation)\n",
    "- Description of the results\n",
    "- Conclusions and directions for future research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1eeb3-8d03-4f59-9784-39501972d178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
