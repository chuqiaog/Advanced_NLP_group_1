{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f9cc43-ffd7-44f7-a33f-78a33800775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086a2bf0-f8bc-4e04-a35c-f69604c66123",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = 'data/en_ewt-up-train.conllu'\n",
    "testfile = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39d315a-ba2b-4f12-a19a-ee08d87e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(conllfile):\n",
    "    \"\"\"\n",
    "    This function read and process the conllu file into list of sentences lists.\n",
    "    \"\"\"\n",
    "    with open(conllfile, 'r', encoding='utf8') as infile:\n",
    "        fulllist, sentlist = [],[]\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if (line != '\\n') & (line.startswith(\"#\") == False): # Not empty and not commented\n",
    "                sentlist.append(line.split())\n",
    "            if line.startswith(\"#\") == True:\n",
    "                sentlist = [i for i in sentlist if i] # Remove empty list\n",
    "                fulllist.append(sentlist)\n",
    "                sentlist = []\n",
    "                continue\n",
    "        res = [ele for ele in fulllist if ele != []] # remove empty list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff1486c-e2be-4487-baae-83132e1d58c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainlist = read_conll(trainfile)\n",
    "testlist = read_conll(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c2910-aac5-49cf-a7dd-7a901f8cfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_list(conlllist):\n",
    "    \"\"\"\n",
    "    This function preprocess the lists into list of sentences list.\n",
    "    Each sentence list is a list of token lists. Each token list have 13 columns.\n",
    "    If a sentence have 0 predicates, the column (list item) 12 and 13 (list[11] and list[12]) are set as None.\n",
    "    If the sentence have multiple predicates, it will be duplicated to align the column number.\n",
    "    \"\"\"\n",
    "    sentlist = []\n",
    "    for sentence in conlllist:\n",
    "        \n",
    "        sents = [ [] for _ in range(50) ]\n",
    "                \n",
    "        \n",
    "        for x in range(len(sentence)): # for components in sentence brings duplicate removal error\n",
    "            components = []\n",
    "            for y in range(len(sentence[x])):\n",
    "                components.append(str(sentence[x][y]))\n",
    "\n",
    "            # First 11 lines\n",
    "            for i in range(0,10):\n",
    "                try:\n",
    "                    tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "                except IndexError:\n",
    "                    tokendict['pred'] = '_'\n",
    "\n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "            if len(components) <= 11: \n",
    "                tokendict['V'] = '_'\n",
    "                tokendict['ARG'] = '_'\n",
    "                tokendict['dup'] = '_'\n",
    "                sents[0].append(tokendict)\n",
    "\n",
    "            # Sentence have one or more predicate\n",
    "            if len(components) > 11: \n",
    "                dup = len(components)-11     \n",
    "                for k in range(0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    if ARGV == 'V':\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = sentence[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print(sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "\n",
    "\n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res\n",
    "\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed797b3-3e11-4c3c-81c4-7a4f7cac6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', '2', 'nsubj', '2:nsubj|9.1:nsubj|10:nsubj', '_', '_', 'ARG0', '_', '_'], ['2', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '0', 'root', '0:root', '_', 'wish.01', 'V', '_', '_'], ['3', 'all', 'all', 'DET', 'DT', '_', '2', 'iobj', '2:iobj', '_', '_', 'ARG2', '_', '_'], ['4', 'happy', 'happy', 'ADJ', 'JJ', 'Degree=Pos', '5', 'amod', '5:amod', '_', '_', '_', 'ARGM-ADJ', '_'], ['5', 'holidays', 'holiday', 'NOUN', 'NNS', 'Number=Plur', '2', 'obj', '2:obj', 'SpaceAfter=No', 'holiday.01', 'ARG1', 'V', '_'], ['6', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['7', 'and', 'and', 'CCONJ', 'CC', '_', '10', 'cc', '9.1:cc|10:cc', '_', '_', '_', '_', '_'], ['8', 'moreso', 'moreso', 'ADV', 'RB', '_', '10', 'orphan', '9.1:advmod', 'SpaceAfter=No', '_', '_', '_', '_'], ['9', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['9.1', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '_', '_', '2:conj:and', 'CopyOf=2'], ['10', 'peace', 'peace', 'NOUN', 'NN', 'Number=Sing', '2', 'conj', '2:conj:and|9.1:obj', '_', 'peace.01', 'ARG1', '_', 'V'], ['11', 'on', 'on', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_'], ['12', 'earth', 'earth', 'NOUN', 'NN', 'Number=Sing', '10', 'nmod', '10:nmod:on', 'SpaceAfter=No', '_', '_', '_', 'ARG1'], ['13', '.', '.', 'PUNCT', '.', '_', '2', 'punct', '2:punct', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_list(trainlist)\n",
    "preprocessed_test = preprocess_list(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd632b5-9f4a-4b9d-9c52-3a55d06f2c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '1',\n",
       "  'form': 'What',\n",
       "  'lemma': 'what',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'WP',\n",
       "  'feats': 'PronType=Int',\n",
       "  'head': '0',\n",
       "  'deprel': 'root',\n",
       "  'deps': '0:root',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 0,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '2',\n",
       "  'form': 'if',\n",
       "  'lemma': 'if',\n",
       "  'upos': 'SCONJ',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '4',\n",
       "  'deprel': 'mark',\n",
       "  'deps': '4:mark',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 0,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '3',\n",
       "  'form': 'Google',\n",
       "  'lemma': 'Google',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '4',\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '4:nsubj',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 0,\n",
       "  'ARG': 'ARG1',\n",
       "  'V': '_'},\n",
       " {'ID': '4',\n",
       "  'form': 'Morphed',\n",
       "  'lemma': 'morph',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBD',\n",
       "  'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin',\n",
       "  'head': '1',\n",
       "  'deprel': 'advcl',\n",
       "  'deps': '1:advcl:if',\n",
       "  'misc': '_',\n",
       "  'pred': 'morph.01',\n",
       "  'dup': 0,\n",
       "  'V': 'V',\n",
       "  'ARG': '_'},\n",
       " {'ID': '5',\n",
       "  'form': 'Into',\n",
       "  'lemma': 'into',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '6',\n",
       "  'deprel': 'case',\n",
       "  'deps': '6:case',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 0,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '6',\n",
       "  'form': 'GoogleOS',\n",
       "  'lemma': 'GoogleOS',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '4',\n",
       "  'deprel': 'obl',\n",
       "  'deps': '4:obl:into',\n",
       "  'misc': 'SpaceAfter=No',\n",
       "  'pred': '_',\n",
       "  'dup': 0,\n",
       "  'ARG': 'ARG2',\n",
       "  'V': '_'},\n",
       " {'ID': '7',\n",
       "  'form': '?',\n",
       "  'lemma': '?',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '.',\n",
       "  'feats': '_',\n",
       "  'head': '4',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '4:punct',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 0,\n",
       "  'V': '_',\n",
       "  'ARG': '_'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21423387-fc45-419d-b804-24435c5893d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from preprocessed list\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG']\n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "821ead1d-3d2b-4f92-8921-814f0407e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, gold_labels = extract_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e96716a-6106-4011-bc7f-07a128510890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '2',\n",
       " 'form': '-',\n",
       " 'lemma': '-',\n",
       " 'upos': 'PUNCT',\n",
       " 'xpos': 'HYPH',\n",
       " 'feats': '_',\n",
       " 'head': '1',\n",
       " 'deprel': 'punct',\n",
       " 'deps': '1:punct',\n",
       " 'misc': 'SpaceAfter=No',\n",
       " 'pred': '_',\n",
       " 'dup': 0,\n",
       " 'V': '_'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5cb68-1629-4908-b000-9ebb3c832268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def create_log_classifier(train_features, train_targets):\n",
    "    logreg = LogisticRegression(max_iter=1000)\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model = logreg.fit(features_vectorized, train_targets) \n",
    "    return model, vec\n",
    "\n",
    "def classify_data(model, vec, inputdata):  \n",
    "    features = extract_feature_and_label(inputdata)[0]\n",
    "    features = vec.transform(features)\n",
    "    predictions = model.predict(features)\n",
    "    return predictions\n",
    "\n",
    "def write_output_file(predictions, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + predictions[i] + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "def logreg(inputdict, outputfile, training_features=training_features, gold_labels=gold_labels):\n",
    "    ml_model, vec = create_log_classifier(training_features, gold_labels)\n",
    "    predictions = classify_data(ml_model, vec, inputdict)\n",
    "    write_output_file(predictions, inputdict, outputfile)\n",
    "\n",
    "logreg(preprocessed_test ,'output/logreg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d197c7-42ad-4814-b17b-51692ef952a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
