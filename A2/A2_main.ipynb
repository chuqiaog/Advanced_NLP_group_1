{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f9cc43-ffd7-44f7-a33f-78a33800775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086a2bf0-f8bc-4e04-a35c-f69604c66123",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = 'data/en_ewt-up-train.conllu'\n",
    "devfile = 'data/en_ewt-up-dev.conllu'\n",
    "testfile = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9dc1d3-3f2d-4029-b658-02c0c0f9fdb0",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39d315a-ba2b-4f12-a19a-ee08d87e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(conllfile):\n",
    "    \"\"\"\n",
    "    This function read and process the conllu file into list of sentences lists.\n",
    "    \"\"\"\n",
    "    with open(conllfile, 'r', encoding='utf8') as infile:\n",
    "        fulllist, sentlist = [],[]\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if (line != '\\n') & (line.startswith(\"#\") == False): # Not empty and not commented\n",
    "                sentlist.append(line.split())\n",
    "            if line.startswith(\"#\") == True:\n",
    "                sentlist = [i for i in sentlist if i] # Remove empty list\n",
    "                fulllist.append(sentlist)\n",
    "                sentlist = []\n",
    "                continue\n",
    "        res = [ele for ele in fulllist if ele != []] # remove empty list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff1486c-e2be-4487-baae-83132e1d58c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainlist = read_conll(trainfile)\n",
    "devlist = read_conll(devfile)\n",
    "testlist = read_conll(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfa092-443d-4d1c-b83d-965b94285b96",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Extract features from dataset and duplicate sentences with multiple predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c2910-aac5-49cf-a7dd-7a901f8cfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_list(conlllist):\n",
    "    \"\"\"\n",
    "    This function preprocess the lists into list of sentences list.\n",
    "    Each sentence list is a list of token lists. Each token list have 13 columns.\n",
    "    If a sentence have 0 predicates, the column (list item) 12 and 13 (list[11] and list[12]) are set as None.\n",
    "    If the sentence have multiple predicates, it will be duplicated to align the column number.\n",
    "    \"\"\"\n",
    "    sentlist = []\n",
    "    for sentence in conlllist:\n",
    "        sents = [ [] for _ in range(50) ] # Initialize a large empty list for multiple predicate sentence    \n",
    "        \n",
    "        for x in range(len(sentence)): # replace 'for components in sentence' that brings duplicate removal error\n",
    "            components = []\n",
    "            for y in range(len(sentence[x])):\n",
    "                components.append(str(sentence[x][y]))\n",
    "\n",
    "            # First 11 lines\n",
    "            for i in range(0,10):\n",
    "                try:\n",
    "                    tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "                except IndexError: # Wrong sentence in the dataset that have no column 11\n",
    "                    tokendict['pred'] = '_'\n",
    "\n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "            if len(components) <= 11: \n",
    "                tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "                sents[0].append(tokendict)\n",
    "\n",
    "            # Sentence have one or more predicate\n",
    "            if len(components) > 11: \n",
    "                dup = len(components)-11 # Times for dpulication\n",
    "                for k in range(0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    # Following conditons change 'pred' (and ARG, V also) entry for duplicated sentence\n",
    "                    if ARGV == 'V':\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = sentence[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print(sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "\n",
    "\n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res\n",
    "\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed797b3-3e11-4c3c-81c4-7a4f7cac6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', '2', 'nsubj', '2:nsubj|9.1:nsubj|10:nsubj', '_', '_', 'ARG0', '_', '_'], ['2', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '0', 'root', '0:root', '_', 'wish.01', 'V', '_', '_'], ['3', 'all', 'all', 'DET', 'DT', '_', '2', 'iobj', '2:iobj', '_', '_', 'ARG2', '_', '_'], ['4', 'happy', 'happy', 'ADJ', 'JJ', 'Degree=Pos', '5', 'amod', '5:amod', '_', '_', '_', 'ARGM-ADJ', '_'], ['5', 'holidays', 'holiday', 'NOUN', 'NNS', 'Number=Plur', '2', 'obj', '2:obj', 'SpaceAfter=No', 'holiday.01', 'ARG1', 'V', '_'], ['6', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['7', 'and', 'and', 'CCONJ', 'CC', '_', '10', 'cc', '9.1:cc|10:cc', '_', '_', '_', '_', '_'], ['8', 'moreso', 'moreso', 'ADV', 'RB', '_', '10', 'orphan', '9.1:advmod', 'SpaceAfter=No', '_', '_', '_', '_'], ['9', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['9.1', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '_', '_', '2:conj:and', 'CopyOf=2'], ['10', 'peace', 'peace', 'NOUN', 'NN', 'Number=Sing', '2', 'conj', '2:conj:and|9.1:obj', '_', 'peace.01', 'ARG1', '_', 'V'], ['11', 'on', 'on', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_'], ['12', 'earth', 'earth', 'NOUN', 'NN', 'Number=Sing', '10', 'nmod', '10:nmod:on', 'SpaceAfter=No', '_', '_', '_', 'ARG1'], ['13', '.', '.', 'PUNCT', '.', '_', '2', 'punct', '2:punct', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_list(trainlist)\n",
    "preprocessed_dev = preprocess_list(devlist)\n",
    "preprocessed_test = preprocess_list(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd632b5-9f4a-4b9d-9c52-3a55d06f2c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '1',\n",
       "  'form': '(',\n",
       "  'lemma': '(',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '-LRB-',\n",
       "  'feats': '_',\n",
       "  'head': '14',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '14:punct',\n",
       "  'misc': 'SpaceAfter=No',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '2',\n",
       "  'form': 'And',\n",
       "  'lemma': 'and',\n",
       "  'upos': 'CCONJ',\n",
       "  'xpos': 'CC',\n",
       "  'feats': '_',\n",
       "  'head': '14',\n",
       "  'deprel': 'cc',\n",
       "  'deps': '14:cc',\n",
       "  'misc': 'SpaceAfter=No',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '3',\n",
       "  'form': ',',\n",
       "  'lemma': ',',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': ',',\n",
       "  'feats': '_',\n",
       "  'head': '14',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '14:punct',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '4',\n",
       "  'form': 'by',\n",
       "  'lemma': 'by',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '6',\n",
       "  'deprel': 'case',\n",
       "  'deps': '6:case',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '5',\n",
       "  'form': 'the',\n",
       "  'lemma': 'the',\n",
       "  'upos': 'DET',\n",
       "  'xpos': 'DT',\n",
       "  'feats': 'Definite=Def|PronType=Art',\n",
       "  'head': '6',\n",
       "  'deprel': 'det',\n",
       "  'deps': '6:det',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '6',\n",
       "  'form': 'way',\n",
       "  'lemma': 'way',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '14',\n",
       "  'deprel': 'obl',\n",
       "  'deps': '14:obl:by',\n",
       "  'misc': 'SpaceAfter=No',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'ARG': 'ARGM-DIS',\n",
       "  'V': '_'},\n",
       " {'ID': '7',\n",
       "  'form': ',',\n",
       "  'lemma': ',',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': ',',\n",
       "  'feats': '_',\n",
       "  'head': '14',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '14:punct',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '8',\n",
       "  'form': 'is',\n",
       "  'lemma': 'be',\n",
       "  'upos': 'AUX',\n",
       "  'xpos': 'VBZ',\n",
       "  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "  'head': '14',\n",
       "  'deprel': 'cop',\n",
       "  'deps': '14:cop',\n",
       "  'misc': '_',\n",
       "  'pred': 'be.01',\n",
       "  'dup': 1,\n",
       "  'V': 'V',\n",
       "  'ARG': '_'},\n",
       " {'ID': '9',\n",
       "  'form': 'anybody',\n",
       "  'lemma': 'anybody',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '14',\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '14:nsubj',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'ARG': 'ARG1',\n",
       "  'V': '_'},\n",
       " {'ID': '10',\n",
       "  'form': 'else',\n",
       "  'lemma': 'else',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': '9',\n",
       "  'deprel': 'amod',\n",
       "  'deps': '9:amod',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '11',\n",
       "  'form': 'just',\n",
       "  'lemma': 'just',\n",
       "  'upos': 'ADV',\n",
       "  'xpos': 'RB',\n",
       "  'feats': '_',\n",
       "  'head': '13',\n",
       "  'deprel': 'advmod',\n",
       "  'deps': '13:advmod',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '12',\n",
       "  'form': 'a',\n",
       "  'lemma': 'a',\n",
       "  'upos': 'DET',\n",
       "  'xpos': 'DT',\n",
       "  'feats': 'Definite=Ind|PronType=Art',\n",
       "  'head': '13',\n",
       "  'deprel': 'det',\n",
       "  'deps': '13:det',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '13',\n",
       "  'form': 'little',\n",
       "  'lemma': 'little',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': '14',\n",
       "  'deprel': 'obl:npmod',\n",
       "  'deps': '14:obl:npmod',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '14',\n",
       "  'form': 'nostalgic',\n",
       "  'lemma': 'nostalgic',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '0',\n",
       "  'deprel': 'root',\n",
       "  'deps': '0:root',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'ARG': 'ARG2',\n",
       "  'V': '_'},\n",
       " {'ID': '15',\n",
       "  'form': 'for',\n",
       "  'lemma': 'for',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '17',\n",
       "  'deprel': 'case',\n",
       "  'deps': '17:case',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '16',\n",
       "  'form': 'the',\n",
       "  'lemma': 'the',\n",
       "  'upos': 'DET',\n",
       "  'xpos': 'DT',\n",
       "  'feats': 'Definite=Def|PronType=Art',\n",
       "  'head': '17',\n",
       "  'deprel': 'det',\n",
       "  'deps': '17:det',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '17',\n",
       "  'form': 'days',\n",
       "  'lemma': 'day',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NNS',\n",
       "  'feats': 'Number=Plur',\n",
       "  'head': '14',\n",
       "  'deprel': 'nmod',\n",
       "  'deps': '14:nmod:for|23:nsubj',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '18',\n",
       "  'form': 'when',\n",
       "  'lemma': 'when',\n",
       "  'upos': 'ADV',\n",
       "  'xpos': 'WRB',\n",
       "  'feats': 'PronType=Rel',\n",
       "  'head': '23',\n",
       "  'deprel': 'advmod',\n",
       "  'deps': '23:advmod',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '19',\n",
       "  'form': 'that',\n",
       "  'lemma': 'that',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'DT',\n",
       "  'feats': 'Number=Sing|PronType=Dem',\n",
       "  'head': '23',\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '17:ref',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '20',\n",
       "  'form': 'was',\n",
       "  'lemma': 'be',\n",
       "  'upos': 'AUX',\n",
       "  'xpos': 'VBD',\n",
       "  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
       "  'head': '23',\n",
       "  'deprel': 'cop',\n",
       "  'deps': '23:cop',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '21',\n",
       "  'form': 'a',\n",
       "  'lemma': 'a',\n",
       "  'upos': 'DET',\n",
       "  'xpos': 'DT',\n",
       "  'feats': 'Definite=Ind|PronType=Art',\n",
       "  'head': '23',\n",
       "  'deprel': 'det',\n",
       "  'deps': '23:det',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '22',\n",
       "  'form': 'good',\n",
       "  'lemma': 'good',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': '23',\n",
       "  'deprel': 'amod',\n",
       "  'deps': '23:amod',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '23',\n",
       "  'form': 'thing',\n",
       "  'lemma': 'thing',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NN',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '17',\n",
       "  'deprel': 'acl:relcl',\n",
       "  'deps': '17:acl:relcl',\n",
       "  'misc': 'SpaceAfter=No',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '24',\n",
       "  'form': '?',\n",
       "  'lemma': '?',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '.',\n",
       "  'feats': '_',\n",
       "  'head': '14',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '14:punct',\n",
       "  'misc': 'SpaceAfter=No',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'},\n",
       " {'ID': '25',\n",
       "  'form': ')',\n",
       "  'lemma': ')',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '-RRB-',\n",
       "  'feats': '_',\n",
       "  'head': '14',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '14:punct',\n",
       "  'misc': '_',\n",
       "  'pred': '_',\n",
       "  'dup': 1,\n",
       "  'V': '_',\n",
       "  'ARG': '_'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90418fea-23c5-473a-80af-cb0d40e6a414",
   "metadata": {},
   "source": [
    "## TO BE IMPLEMENTED: GET OTHER FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26028649-a33d-4317-94cb-6110ed3b5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(preplist):\n",
    "    \"\"\"\n",
    "    This function creates extra features by dependency parsing, using a preprocessed list.\n",
    "    \"\"\"\n",
    "    sent_with_feature = []\n",
    "    \n",
    "    for sentence in preplist:\n",
    "        \n",
    "        # Extract sentence text\n",
    "        sentence_text = []\n",
    "        for dict in sentence:\n",
    "            sentence_text += dict['form']\n",
    "            \n",
    "        # IMPLEMENT HERE: Extract sentence features\n",
    "        #feat1, feat2, ... = extract_parsing_features(sentence_text)\n",
    "\n",
    "        # Add features back to dict\n",
    "        newsent = []\n",
    "        for dict in sentence:\n",
    "            newdict = copy.deepcopy(dict) # Avoid changing the original file\n",
    "            #newdict['feat1'], newdict['feat2'], ... = feat1, feat2\n",
    "            newsent.append(newdict)\n",
    "\n",
    "        sent_with_feature.append(newsent)\n",
    "\n",
    "    return sent_with_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca47588-3f9d-433d-b393-ce49762716e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parsing_features():\n",
    "    \"\"\" TO BE IMPLEMENTED \u0000 \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63642da-e721-4032-bf7b-d7a66972117a",
   "metadata": {},
   "source": [
    "## Single classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a8678-9ecb-404c-bee7-f7bd6d655860",
   "metadata": {},
   "source": [
    "### Extract training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21423387-fc45-419d-b804-24435c5893d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from extracted feature list of dicts.\n",
    "    It will flattern list of sentences into list of tokens.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821ead1d-3d2b-4f92-8921-814f0407e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, gold_labels = extract_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa7317-724c-444f-9054-fe94cc4c40fa",
   "metadata": {},
   "source": [
    "### Create single logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac80732a-cb99-48ae-a46a-f9b45ea8183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def create_log_classifier(train_features, train_targets, max_iter):\n",
    "    logreg = LogisticRegression(max_iter=max_iter)\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model = logreg.fit(features_vectorized, train_targets) \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cdce8e0-8caf-49bd-8e5c-4289172a2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_single, vec_single = create_log_classifier(training_features, gold_labels, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc0d5f-2948-4e83-b81c-d14e02683931",
   "metadata": {},
   "source": [
    "### Predict with single logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b5cb68-1629-4908-b000-9ebb3c832268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, features):  \n",
    "    features = vec.transform(features)\n",
    "    predictions = model.predict(features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55bf2eed-63c3-49d9-9571-ca461620bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ea878-cd8c-4e8c-8eca-3ca2bc19362f",
   "metadata": {},
   "source": [
    "### Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d197c7-42ad-4814-b17b-51692ef952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_file(predictions, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c471942d-587a-4f50-b62f-e4fbd9905245",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/singlelogreg.csv'\n",
    "write_output_file(single_predictions, training_features, gold_labels, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7bfa1-d6a7-45e5-942c-eaf03f833ee6",
   "metadata": {},
   "source": [
    "## Double classifier\n",
    "First classify is_ARG, then classify ARG_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d02f4bdc-f814-4ea5-b27c-a7d5d14cda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_is_ARG_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from preprocessed list\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        data.append(newdict)\n",
    "        \n",
    "        if dict['ARG'] != '_':\n",
    "            targets.append(True)\n",
    "        else:\n",
    "            targets.append(False)\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95d6f97b-efd1-4cee-b453-388bb2acbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step1, gold_labels_step1 = extract_is_ARG_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb1b009-4f8a-47b3-a82c-0cd875db9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_double_1, vec_double_1 = create_log_classifier(training_features_step1, gold_labels_step1, 100)\n",
    "\n",
    "using_test_set_1, test_gold_1 = extract_is_ARG_feature_and_label(preprocessed_test)\n",
    "predictions_1 = classify_data(model_double_1, vec_double_1, using_test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9c606af-60bd-4cd2-937c-146e95b44fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ARG_type_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract ARG_type feature from the training set.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    \n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        if dict['ARG'] != '_':\n",
    "            newdict['is_ARG'] = 'True'\n",
    "        else:\n",
    "            newdict['is_ARG'] = 'False'\n",
    "        \n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba39c51c-cafc-45d2-b319-834158ebeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step2, gold_labels_step2 = extract_ARG_type_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56e1a8a3-a7de-4519-813b-cd3abf2895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ARG_type_feature_and_label_with_prediction(preplist, predictions_1):\n",
    "    \"\"\"\n",
    "    This function add result from the first classifier to the feature list for the test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    \n",
    "    for dict, predictions in zip(flatlist, predictions_1):\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        newdict['is_ARG'] = str(predictions)\n",
    "        \n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9999d3ee-6225-4fe4-b4b5-ab61098b54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_double_2, vec_double_2 = create_log_classifier(training_features_step2, gold_labels_step2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7a6c984-3698-4f69-acf1-b2a32133fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_2, test_gold_2 = extract_ARG_type_feature_and_label_with_prediction(preprocessed_test, predictions_1)\n",
    "\n",
    "predictions_2 = classify_data(model_double_2, vec_double_2, using_test_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76f23d02-b5ed-4945-84a0-a6cff4e1993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/doublelogreg.csv'\n",
    "write_output_file(predictions_2, training_features_step2, gold_labels_step2, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3a1b7-14c0-44ed-8145-50660f5ff510",
   "metadata": {},
   "source": [
    "## GPU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024ecdb-cbd8-48a6-9f2c-47f16c24c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "from cuml import LogisticRegression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
