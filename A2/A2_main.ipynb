{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f9cc43-ffd7-44f7-a33f-78a33800775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086a2bf0-f8bc-4e04-a35c-f69604c66123",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = 'data/en_ewt-up-train.conllu'\n",
    "devfile = 'data/en_ewt-up-dev.conllu'\n",
    "testfile = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9dc1d3-3f2d-4029-b658-02c0c0f9fdb0",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39d315a-ba2b-4f12-a19a-ee08d87e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(conllfile):\n",
    "    \"\"\"\n",
    "    This function read and process the conllu file into list of sentences lists.\n",
    "    \"\"\"\n",
    "    with open(conllfile, 'r', encoding='utf8') as infile:\n",
    "        fulllist, sentlist = [],[]\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if (line != '\\n') & (line.startswith(\"#\") == False): # Not empty and not commented\n",
    "                sentlist.append(line.split())\n",
    "            if line.startswith(\"#\") == True:\n",
    "                sentlist = [i for i in sentlist if i] # Remove empty list\n",
    "                fulllist.append(sentlist)\n",
    "                sentlist = []\n",
    "                continue\n",
    "        res = [ele for ele in fulllist if ele != []] # remove empty list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff1486c-e2be-4487-baae-83132e1d58c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainlist = read_conll(trainfile)\n",
    "devlist = read_conll(devfile)\n",
    "testlist = read_conll(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfa092-443d-4d1c-b83d-965b94285b96",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Extract features from dataset and duplicate sentences with multiple predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c2910-aac5-49cf-a7dd-7a901f8cfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_list(conlllist):\n",
    "    \"\"\"\n",
    "    This function preprocess the lists into list of sentences list.\n",
    "    Each sentence list is a list of token lists. Each token list have 13 columns.\n",
    "        If a sentence have 0 predicates, the column (list item) 12 and 13 (list[11] and list[12]) are set as None.\n",
    "        If the sentence have multiple predicates, it will be duplicated to align the column number.\n",
    "        If a sentence does not have record on line 11, it will be filled with '_'\n",
    "    \"\"\"\n",
    "    sentlist = []\n",
    "    for sentence in conlllist:\n",
    "        sents = [ [] for _ in range(50) ] # Initialize a large empty list for multiple predicate sentence    \n",
    "        \n",
    "        for x in range(len(sentence)): # replace 'for components in sentence' that brings duplicate removal error\n",
    "            components = []\n",
    "            for y in range(len(sentence[x])):\n",
    "                components.append(str(sentence[x][y]))\n",
    "\n",
    "            # First 11 lines\n",
    "            for i in range(0,10):\n",
    "                try:\n",
    "                    tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "                except IndexError: # Wrong sentence in the dataset that have no column 11\n",
    "                    tokendict['pred'] = '_'\n",
    "\n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "            if len(components) <= 11: \n",
    "                tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "                sents[0].append(tokendict)\n",
    "\n",
    "            # Sentence have one or more predicate\n",
    "            if len(components) > 11: \n",
    "                dup = len(components)-11 # Times for dpulication\n",
    "                for k in range(0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    # Following conditons change 'pred' (and ARG, V also) entry for duplicated sentence\n",
    "                    if ARGV == 'V':\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = sentence[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print('The following sentence contains error:',sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "\n",
    "\n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res\n",
    "\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed797b3-3e11-4c3c-81c4-7a4f7cac6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following sentence contains error: [['1', 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', '2', 'nsubj', '2:nsubj|9.1:nsubj|10:nsubj', '_', '_', 'ARG0', '_', '_'], ['2', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '0', 'root', '0:root', '_', 'wish.01', 'V', '_', '_'], ['3', 'all', 'all', 'DET', 'DT', '_', '2', 'iobj', '2:iobj', '_', '_', 'ARG2', '_', '_'], ['4', 'happy', 'happy', 'ADJ', 'JJ', 'Degree=Pos', '5', 'amod', '5:amod', '_', '_', '_', 'ARGM-ADJ', '_'], ['5', 'holidays', 'holiday', 'NOUN', 'NNS', 'Number=Plur', '2', 'obj', '2:obj', 'SpaceAfter=No', 'holiday.01', 'ARG1', 'V', '_'], ['6', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['7', 'and', 'and', 'CCONJ', 'CC', '_', '10', 'cc', '9.1:cc|10:cc', '_', '_', '_', '_', '_'], ['8', 'moreso', 'moreso', 'ADV', 'RB', '_', '10', 'orphan', '9.1:advmod', 'SpaceAfter=No', '_', '_', '_', '_'], ['9', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['9.1', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '_', '_', '2:conj:and', 'CopyOf=2'], ['10', 'peace', 'peace', 'NOUN', 'NN', 'Number=Sing', '2', 'conj', '2:conj:and|9.1:obj', '_', 'peace.01', 'ARG1', '_', 'V'], ['11', 'on', 'on', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_'], ['12', 'earth', 'earth', 'NOUN', 'NN', 'Number=Sing', '10', 'nmod', '10:nmod:on', 'SpaceAfter=No', '_', '_', '_', 'ARG1'], ['13', '.', '.', 'PUNCT', '.', '_', '2', 'punct', '2:punct', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_list(trainlist)\n",
    "preprocessed_dev = preprocess_list(devlist)\n",
    "preprocessed_test = preprocess_list(testlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f412f-7a63-4a83-8cbc-85cad61e0b19",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d164bd9a-a3d2-4fa1-9ae5-8f409eea2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(preplist):\n",
    "    \"\"\"\n",
    "    This function clacluates basic statistics for the dataset.\n",
    "    Statistics: num_sent, num_token, sent_without_pred, duplicated_sent, num_pred, num_arg\n",
    "    \"\"\"\n",
    "    num_sent = len(preplist)\n",
    "    num_token, num_pred, num_arg, without_pred, duplicated_sent = 0,0,0,0,0\n",
    "    for sentence in preplist:\n",
    "        with_pred, dup_sent = False, False\n",
    "        num_token_sent, num_pred_sent, num_arg_sent = 0,0,0\n",
    "        for token in sentence:\n",
    "            if token['V'] != '_':\n",
    "                with_pred = True\n",
    "                num_pred_sent += 1\n",
    "            if (token['dup'] != 0) & (token['dup'] != '_'): dup_sent = True\n",
    "            if token['ARG'] != '_': num_arg_sent += 1\n",
    "            num_token_sent += 1\n",
    "        num_token += num_token_sent\n",
    "        num_pred += num_pred_sent\n",
    "        num_arg += num_arg_sent\n",
    "        if with_pred == False: without_pred += 1 \n",
    "        if dup_sent == True: duplicated_sent += 1\n",
    "\n",
    "    print('Sentence  Token     Predicates  Arguments  sent_without_pred  dup_sent')\n",
    "    print(num_sent,'\\t', num_token,'\\t', num_pred,'\\t', num_arg,'\\t\\t', without_pred,'\\t\\t', duplicated_sent)\n",
    "    print('PERCENTAGE    dup    without_pred    Predicate    Argument')\n",
    "    print('           ','{:.3f}'.format(100*duplicated_sent/num_sent),'\\t', '{:.3f}'.format(100*without_pred/num_sent),'\\t  ', \n",
    "          '{:.3f}'.format(100*num_pred/num_token),'\\t', '{:.3f}'.format(100*num_arg/num_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a252d040-11b3-46eb-8398-7e3b494879f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  Token     Predicates  Arguments  sent_without_pred  dup_sent\n",
      "42466 \t 1035797 \t 40496 \t 82436 \t\t 1990 \t\t 29924\n",
      "PERCENTAGE    dup    without_pred    Predicate    Argument\n",
      "            70.466 \t 4.686 \t   3.910 \t 7.959\n"
     ]
    }
   ],
   "source": [
    "get_statistics(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b233e4-c666-4905-9b73-c28a79c2d5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  Token     Predicates  Arguments  sent_without_pred  dup_sent\n",
      "5328 \t 103046 \t 4792 \t 9420 \t\t 539 \t\t 3252\n",
      "PERCENTAGE    dup    without_pred    Predicate    Argument\n",
      "            61.036 \t 10.116 \t   4.650 \t 9.142\n"
     ]
    }
   ],
   "source": [
    "get_statistics(preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12cef26b-0d7a-4d2b-9eb8-8628bb9dfb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gold_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 2\u001b[0m train_label_count,test_label_count \u001b[38;5;241m=\u001b[39m Counter(\u001b[43mgold_labels\u001b[49m), Counter(test_gold)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_label_count[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m], test_label_count[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m train_labels, train_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_label_count\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;28mlist\u001b[39m(train_label_count\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gold_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train_label_count,test_label_count = Counter(gold_labels), Counter(test_gold)\n",
    "del train_label_count['_'], test_label_count['_']\n",
    "train_labels, train_values = list(train_label_count.keys()), list(train_label_count.values())\n",
    "test_labels, test_values = list(test_label_count.keys()), list(test_label_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b17cfa-5631-4a5b-b6f6-ab8f74d20a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43780a-9780-4026-829f-3deb9ea1866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(train_values,\n",
    "        labels = [n if v > sum(train_values) * 0.02 else '' for n, v in zip(train_labels, train_values)],\n",
    "        autopct=lambda p : '{:.2f}% '.format(p,p * sum(train_values)/100) if p > 10 else '')\n",
    "plt.title(\"Train\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(test_values,\n",
    "        labels = [n if v > sum(test_values) * 0.02 else '' for n, v in zip(test_labels, test_values)],\n",
    "        autopct=lambda p : '{:.2f}% '.format(p,p * sum(test_values)/100) if p > 10 else '')\n",
    "plt.title(\"Test\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90418fea-23c5-473a-80af-cb0d40e6a414",
   "metadata": {},
   "source": [
    "FEATURE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11b83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def dep_dist_to_head(input_text, conll_keys):\n",
    "    '''\n",
    "    This function calculates the dependency distance from token to head.\n",
    "    Input: flat python string\n",
    "    Return: A list of dict with {'token':token, 'dist_to_head':its distance to head}\n",
    "            If negative, then the token is before the head.\n",
    "    '''\n",
    "    doc = nlp(input_text)\n",
    "    token_full_info = doc.to_json()['tokens']\n",
    "    dist_to_head = dict()\n",
    "    token_combination = ''\n",
    "    combining_tokens = False\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        \n",
    "        if doc[i].text.strip() not in conll_keys or combining_tokens:\n",
    "            combining_tokens = True\n",
    "            token_combination += doc[i].text.strip()\n",
    "            if token_combination in conll_keys:\n",
    "                dist_to_head[token_combination] = token_full_info[i]['id']-token_full_info[i]['head']\n",
    "                token_combination = ''\n",
    "                combining_tokens = False\n",
    "        elif doc[i].text.strip() in conll_keys and not combining_tokens:\n",
    "            dist_to_head[doc[i].text] = token_full_info[i]['id']-token_full_info[i]['head']\n",
    "            token_combination = ''\n",
    "\n",
    "    return dist_to_head\n",
    "\n",
    "def named_entity_recognition(input_text, conll_keys):\n",
    "    \"\"\"\n",
    "    function to return all named entities from an input text.\n",
    "        :input: flat python string\n",
    "        :output: list of all named entities present in the input.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(input_text)\n",
    "    ents = doc.ents\n",
    "    NER_map = dict()\n",
    "    token_combination = ''\n",
    "    combining_tokens = False\n",
    "    combined_is_NER = False\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        \n",
    "        if doc[i].text.strip() not in conll_keys or combining_tokens:\n",
    "            combining_tokens = True\n",
    "            token_combination += doc[i].text.strip()\n",
    "            if doc[i].text in ents:\n",
    "                combined_is_NER = True\n",
    "            if token_combination in conll_keys:\n",
    "                NER_map[token_combination] = combined_is_NER\n",
    "                token_combination = ''\n",
    "                combining_tokens = False\n",
    "                combined_is_NER = False\n",
    "\n",
    "        elif doc[i].text.strip() in conll_keys and not combining_tokens:\n",
    "            if doc[i] in ents:\n",
    "                NER_map[doc[i].text] = True\n",
    "            else:\n",
    "                NER_map[doc[i].text] = False\n",
    "            token_combination = ''\n",
    "\n",
    "    return NER_map\n",
    "\n",
    "  \n",
    "    #return [wordnet.synsets(str(token)) for token in doc if token != '']\n",
    "\n",
    "def syntactic_head(input_text, conll_keys):\n",
    "    '''\n",
    "    This function finds the syntactic head for each word in the input text.\n",
    "    Input: flat python string\n",
    "    Return: heads (list): List of dictionaries with {word:head} relation\n",
    "    '''\n",
    "    doc = nlp(input_text)\n",
    "    head_map = dict()\n",
    "    token_combination = ''\n",
    "    combining_tokens = False\n",
    "        #heads.append({token.text: token.head.text})  # Add {word:head} relation to the list\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        \n",
    "        if doc[i].text.strip() not in conll_keys or combining_tokens:\n",
    "            combining_tokens = True\n",
    "            token_combination += doc[i].text.strip()\n",
    "            if token_combination in conll_keys:\n",
    "                head_map[token_combination] = doc[i].head.text\n",
    "                token_combination = ''\n",
    "                combining_tokens = False\n",
    "        elif doc[i].text.strip() in conll_keys and not combining_tokens:\n",
    "            head_map[doc[i].text] = doc[i].head.text\n",
    "            token_combination = ''\n",
    "\n",
    "    return head_map\n",
    "\n",
    "def extract_trigram(conll_keys):\n",
    "\n",
    "    '''\n",
    "    This function creates bigrams of the given text\n",
    "    Input: flat python string\n",
    "    Return: A list of dict with {'token':token, 'bigram': bigram}\n",
    "    '''\n",
    "    trigram_error = []\n",
    "    try:\n",
    "        trigram_dict = dict()\n",
    "        trigram_dict[conll_keys[0]] = [f'SOS {conll_keys[0]} {conll_keys[1]}']\n",
    "        for i in range(1,len(conll_keys)-1):\n",
    "            trigram_dict[conll_keys[i]] = f'{conll_keys[i-1]} {conll_keys[i]} {conll_keys[i+1]}'\n",
    "        trigram_dict[conll_keys[-1]] = f'{conll_keys[-2]} {conll_keys[-1]} EOS'\n",
    "\n",
    "        return trigram_dict\n",
    "    except:\n",
    "        trigram_error.append(conll_keys)\n",
    "        # return trigram_error\n",
    "        \n",
    "def follows_predicate(predicate, conll_keys):\n",
    "    '''\n",
    "    This function checks wether a word immediately follows (comes after) the predicate of the sentence\n",
    "    '''\n",
    "    bool_map = dict.fromkeys(conll_keys, False)\n",
    "\n",
    "    #if there is no predicate in the sentence, return all values as False straight away\n",
    "    if predicate is None:\n",
    "        return bool_map\n",
    "    \n",
    "    for i, word in enumerate(conll_keys):\n",
    "\n",
    "        if i == 0:\n",
    "            continue\n",
    "        elif conll_keys[i-1] == predicate:\n",
    "            bool_map[word] = True\n",
    "\n",
    "    return bool_map\n",
    "\n",
    "def leads_predicate(predicate, conll_keys):\n",
    "    '''\n",
    "    This function checks wether a word immediately leads (comes before) the predicate of the sentence\n",
    "    '''\n",
    "    bool_map = dict.fromkeys(conll_keys, False)\n",
    "\n",
    "    #if there is no predicate in the sentence, return all values as False straight away\n",
    "    #otherwise if the predicate is at the start there cannot be a leading token, so return all as False\n",
    "    if predicate is None or conll_keys[0] == predicate:\n",
    "        return bool_map\n",
    "\n",
    "    for i, word in enumerate(conll_keys):\n",
    "        if conll_keys[i+1] == predicate:\n",
    "            bool_map[word] = True\n",
    "            break\n",
    "    return bool_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26028649-a33d-4317-94cb-6110ed3b5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(preplist):\n",
    "    \"\"\"\n",
    "    This function creates extra features by dependency parsing, using a preprocessed list.\n",
    "    Also remove the features that are not needed from preprocessed dataset.\n",
    "    \"\"\"\n",
    "    sent_with_feature = []\n",
    "    \n",
    "    for sentence in tqdm(preplist):\n",
    "        # Extract sentence text\n",
    "        sentence_text = ' '.join(word['form'] for word in sentence)\n",
    "        conll_keys = [word['form'] for word in sentence]\n",
    "        predicate = [word['form'] for word in sentence if word['pred'] != '_']\n",
    "        if len(predicate) == 0:\n",
    "            predicate = None\n",
    "        else:\n",
    "            predicate = predicate[0]\n",
    "\n",
    "        try: # try processing each sentence and enriching, this only fails for broken data (URL instead of sentence, or single word conll entries)\n",
    "            dist_to_head_map = dep_dist_to_head(sentence_text, conll_keys)\n",
    "            NER_map = named_entity_recognition(sentence_text, conll_keys)\n",
    "            syntactic_head_map = syntactic_head(sentence_text, conll_keys)\n",
    "            trigram_map = extract_trigram(conll_keys)\n",
    "            following_predicate_map = follows_predicate(predicate, conll_keys)\n",
    "            leading_predicate_map = leads_predicate(predicate, conll_keys)\n",
    "        except:\n",
    "            continue\n",
    "        # Add features back to dict\n",
    "        news_sentence_dict = []\n",
    "        for word_dict in sentence:\n",
    "            new_word_dict = copy.deepcopy(word_dict) # Avoid changing the original file\n",
    "            \n",
    "            try:\n",
    "                # add the distance for each word to head\n",
    "                new_word_dict['dist_to_head'] = dist_to_head_map[word_dict['form']]                \n",
    "                # add value for named entity of each word\n",
    "                new_word_dict['is_named_entity'] = NER_map[word_dict['form']]\n",
    "                # add syntactic head\n",
    "                new_word_dict['syntactic_head'] = syntactic_head_map[word_dict['form']]\n",
    "                # add word trigram\n",
    "                new_word_dict['trigram'] = trigram_map[word_dict['form']]\n",
    "                # following the predicate\n",
    "                new_word_dict['follows_pred'] = following_predicate_map[word_dict['form']]\n",
    "                # preceeding the predicate\n",
    "                new_word_dict['preceeds_pred'] = leading_predicate_map[word_dict['form']]\n",
    "\n",
    "                del new_word_dict['deprel'],new_word_dict['feats'],new_word_dict['misc'] # remove unused\n",
    "\n",
    "                news_sentence_dict.append(new_word_dict)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "                \n",
    "        sent_with_feature.append(news_sentence_dict)\n",
    "\n",
    "    return sent_with_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53195008-9161-4c64-9b83-0bb5a2e85c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc70053554d47d8b9c0712afad29453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = create_features(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f40c2de7-1065-4b98-ab80-5ae73e44f507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e583af139b4fd38b8c1e1c0d132089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features =  create_features(preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63642da-e721-4032-bf7b-d7a66972117a",
   "metadata": {},
   "source": [
    "## Single classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a8678-9ecb-404c-bee7-f7bd6d655860",
   "metadata": {},
   "source": [
    "### Extract training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21423387-fc45-419d-b804-24435c5893d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from extracted feature list of dicts.\n",
    "    It will flattern list of sentences into list of tokens.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea400e8-fa50-4d3d-8ca0-dd8865517d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '1',\n",
       " 'form': 'Al',\n",
       " 'lemma': 'Al',\n",
       " 'upos': 'PROPN',\n",
       " 'xpos': 'NNP',\n",
       " 'head': '0',\n",
       " 'deps': '0:root',\n",
       " 'pred': '_',\n",
       " 'dup': 0,\n",
       " 'V': '_',\n",
       " 'dist_to_head': -2,\n",
       " 'is_named_entity': False,\n",
       " 'syntactic_head': 'Zaman',\n",
       " 'trigram': ['SOS Al -'],\n",
       " 'follows_pred': False,\n",
       " 'preceeds_pred': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821ead1d-3d2b-4f92-8921-814f0407e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, gold_labels = extract_feature_and_label(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa7317-724c-444f-9054-fe94cc4c40fa",
   "metadata": {},
   "source": [
    "### Create single logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac80732a-cb99-48ae-a46a-f9b45ea8183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def create_log_classifier(train_features, train_targets, max_iter):\n",
    "    logreg = LogisticRegression(max_iter=max_iter)\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model = logreg.fit(features_vectorized, train_targets) \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1cdce8e0-8caf-49bd-8e5c-4289172a2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_single, vec_single = create_log_classifier(training_features, gold_labels, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc0d5f-2948-4e83-b81c-d14e02683931",
   "metadata": {},
   "source": [
    "### Predict with single logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1b5cb68-1629-4908-b000-9ebb3c832268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, features):  \n",
    "    features = vec.transform(features)\n",
    "    predictions = model.predict(features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55bf2eed-63c3-49d9-9571-ca461620bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(test_features)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ea878-cd8c-4e8c-8eca-3ca2bc19362f",
   "metadata": {},
   "source": [
    "### Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "57d197c7-42ad-4814-b17b-51692ef952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_file(predictions, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w', encoding='utf8')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c471942d-587a-4f50-b62f-e4fbd9905245",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/singlelogreg.csv'\n",
    "write_output_file(single_predictions, using_test_set, test_gold, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7bfa1-d6a7-45e5-942c-eaf03f833ee6",
   "metadata": {},
   "source": [
    "## Double classifier\n",
    "First classify is_ARG, then classify ARG_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d02f4bdc-f814-4ea5-b27c-a7d5d14cda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_is_ARG_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from preprocessed list\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        data.append(newdict)\n",
    "        \n",
    "        if dict['ARG'] != '_':\n",
    "            targets.append(True)\n",
    "        else:\n",
    "            targets.append(False)\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f9251-ad95-4f2a-b380-d3d5d4140a65",
   "metadata": {},
   "source": [
    "**Here, we use less feature for step 1 to reduce overfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "02139466-c1ea-4aa5-9ac5-97a81b30f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducing_features(inputfeature):\n",
    "    \"\"\"\n",
    "    This function reduce the amount of feature used. Input is the ready-to-use feature dict.\n",
    "    \"\"\"\n",
    "    newfeature = copy.deepcopy(inputfeature)\n",
    "    for newdicts in newfeature:\n",
    "        del newdicts['ID'], newdicts['lemma'], newdicts['dup'], newdicts['trigram'], newdicts['is_named_entity']\n",
    "\n",
    "    return newfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95d6f97b-efd1-4cee-b453-388bb2acbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step1, gold_labels_step1 = extract_is_ARG_feature_and_label(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6321564b-ed74-4f71-a9e9-8ae87b9c39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_training_features_step1 = reducing_features(training_features_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4cb1b009-4f8a-47b3-a82c-0cd875db9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_double_1, vec_double_1 = create_log_classifier(reduced_training_features_step1, gold_labels_step1, 100)\n",
    "\n",
    "using_test_set_1, test_gold_1 = extract_is_ARG_feature_and_label(test_features)\n",
    "predictions_1 = classify_data(model_double_1, vec_double_1, using_test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9c606af-60bd-4cd2-937c-146e95b44fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ARG_type_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract ARG_type feature from the training set.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    \n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        if dict['ARG'] != '_':\n",
    "            newdict['is_ARG'] = 'True'\n",
    "        else:\n",
    "            newdict['is_ARG'] = 'False'\n",
    "        \n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba39c51c-cafc-45d2-b319-834158ebeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step2, gold_labels_step2 = extract_ARG_type_feature_and_label(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "56e1a8a3-a7de-4519-813b-cd3abf2895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ARG_type_feature_and_label_with_prediction(preplist, predictions_1):\n",
    "    \"\"\"\n",
    "    This function add result from the first classifier to the feature list for the test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    \n",
    "    for dict, predictions in zip(flatlist, predictions_1):\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        newdict['is_ARG'] = str(predictions)\n",
    "        \n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9999d3ee-6225-4fe4-b4b5-ab61098b54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_double_2, vec_double_2 = create_log_classifier(training_features_step2, gold_labels_step2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7a6c984-3698-4f69-acf1-b2a32133fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_2, test_gold_2 = extract_ARG_type_feature_and_label_with_prediction(test_features, predictions_1)\n",
    "\n",
    "predictions_2 = classify_data(model_double_2, vec_double_2, using_test_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76f23d02-b5ed-4945-84a0-a6cff4e1993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/doublelogreg.csv'\n",
    "write_output_file(predictions_2, using_test_set_2, test_gold_2, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3a1b7-14c0-44ed-8145-50660f5ff510",
   "metadata": {},
   "source": [
    "## GPU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a024ecdb-cbd8-48a6-9f2c-47f16c24c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "103f4734-a5e8-43f3-8a3b-fe42a7f8afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dict_vectorize(train_features):\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    return vec, features_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "592f885b-0ee6-460d-b41c-518dc328f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings_dict(inputlist):\n",
    "    category_list = copy.deepcopy(inputlist)\n",
    "    category_list.append(None)\n",
    "    map_dict = dict(zip(set(category_list), range(len(set(category_list)))))\n",
    "    map_dict_reverse = {v: k for k, v in map_dict.items()}\n",
    "    return map_dict, map_dict_reverse\n",
    "\n",
    "def numerical_mapping(category_list, list_dict):\n",
    "    numerical_list = [list_dict[line] for line in category_list]\n",
    "    return numerical_list\n",
    "\n",
    "mydict, mydict_rev = get_mappings_dict(gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "97f45222-cc0b-4f33-97e3-d83b45d2344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_feature_and_gold(feat_vec, gold_labels=gold_labels, mydict=mydict):\n",
    "    # This function convert array to cuml required cp_array type\n",
    "    cpfeature = cp.sparse.csr_matrix(feat_vec)\n",
    "    cpgold = [mydict[line] for line in gold_labels]\n",
    "    cpgold = cp.array(cpgold, dtype=cp.int64)\n",
    "    return cpfeature, cpgold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d07fd814-2406-4ab1-8643-6dee8c98cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data_with_rewrite(using_test_set, vec, model, dict=mydict_rev):  \n",
    "    features = vec.transform(using_test_set)\n",
    "    predictions = model.predict(features)\n",
    "    rw_predictions = [dict[line] for line in predictions]\n",
    "    return rw_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935008c-19e9-4558-872b-df61df336e63",
   "metadata": {},
   "source": [
    "### Single classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "291f2fae-eb84-4e2b-94d9-d55c914eb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_single, feat_vec_single = dict_vectorize(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df689fd8-ffe8-409b-9f5d-9f1f394be76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_single, y_single = cp_feature_and_gold(feat_vec_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "50d16b49-e5f4-4c53-9b20-c8adfb5e31b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_single = LogisticRegression(max_iter=50000,class_weight='balanced')\n",
    "reg_single.fit(X_single,y_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ad92825a-2914-40ba-89ec-93b1eb92f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(test_features)\n",
    "single_pred = classify_data_with_rewrite(using_test_set,vec_single,reg_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "da5ab6ce-d9da-45c5-9422-7ae342056074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cp_output_file(predictions, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w', encoding='utf8')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c37f4b9-ea66-4c40-802b-24ac4cacdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/GPUsinglelogreg.csv'\n",
    "write_cp_output_file(single_pred, using_test_set, test_gold, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073f2d3-c490-426a-b03c-09d9c0673ffc",
   "metadata": {},
   "source": [
    "### Double classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "900201a6-a65c-4fb6-a332-f5f208641903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec_double_1, feat_vec_double_1 = dict_vectorize(training_features_step1)\n",
    "vec_double_1, feat_vec_double_1 = dict_vectorize(reduced_training_features_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "000bb280-c393-46af-a88c-da189c463b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_double_1 = cp.sparse.csr_matrix(feat_vec_double_1)\n",
    "y_double_1 = cp.array(gold_labels_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c2a48aa9-f6f3-4d28-b410-436d70b93292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_double_1 = LogisticRegression(max_iter=5000)\n",
    "reg_double_1.fit(X_double_1,y_double_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eeef5d72-0e0c-455d-ba1d-0b10ff7da8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_1, test_gold_1 = extract_is_ARG_feature_and_label(test_features)\n",
    "using_test_set_1_vec = vec_double_1.transform(using_test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3a43e693-fc43-47ea-b72c-8dd5196d2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_pred_1 = reg_double_1.predict(using_test_set_1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba487d3-3e2a-4255-8900-536a191bebab",
   "metadata": {},
   "source": [
    "### Quick overfit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f2f129c5-178f-4d7a-b01e-1a57b8ccd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report_double_1 = classification_report(test_gold_1, double_pred_1, digits = 7, target_names = ['True', 'False'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eb7ab129-cac7-4e23-b2c2-215c1ae9ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True  0.9302344 0.9851398 0.9569002     93404\n",
      "       False  0.6443761 0.2670986 0.3776560      9416\n",
      "\n",
      "    accuracy                      0.9193834    102820\n",
      "   macro avg  0.7873053 0.6261192 0.6672781    102820\n",
      "weighted avg  0.9040562 0.9193834 0.9038544    102820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_double_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6de4d-767e-4b6a-96f4-dcef60788f59",
   "metadata": {},
   "source": [
    "#### Second classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "932f2cc0-cffa-43e4-865e-09a8225992d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step2, gold_labels_step2 = extract_ARG_type_feature_and_label(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "28916dcf-8c74-4853-9fc4-20a0ceaae721",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_double_2, feat_vec_double_2 = dict_vectorize(training_features_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b3ace737-05db-44aa-9e9d-1eeccbdb5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_double_2, y_double_2 = cp_feature_and_gold(feat_vec_double_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bd35359a-d0fd-4f84-a28e-18ece934f569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034330,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_double_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ee204e6b-2c4f-4eaf-909c-4e5155bd8211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-LOC',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-MOD',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-GOL',\n",
       " 'ARG1',\n",
       " '_',\n",
       " 'ARGM-TMP',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-LOC',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG2',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG2',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-ADV',\n",
       " 'ARG1',\n",
       " 'ARGM-MOD',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG2',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-MNR',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-ADV',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-TMP',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-TMP',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-MNR',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-LOC',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-LVB',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-PRR',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG4',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-ADV',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " 'ARGM-NEG',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " 'ARG2',\n",
       " '_',\n",
       " 'ARGM-EXT',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " 'R-ARG0',\n",
       " '_',\n",
       " 'ARGM-MNR',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG2',\n",
       " '_',\n",
       " 'ARGM-LOC',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-ADJ',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG0',\n",
       " '_',\n",
       " '_',\n",
       " 'ARG1',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " 'ARGM-TMP',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " '_',\n",
       " ...]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7dd10-9a61-4ca5-8d11-6768b9927c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d6394-ba01-42fb-9e54-fc5d1b4465ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f543fd-a1b5-4e71-a882-de85780985f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd7b32-f416-4b65-9660-ffb2e05e47e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8fa8c01b-90be-4ec7-9439-fff021b6d0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_double_2 = LogisticRegression(max_iter=50000,class_weight='balanced')\n",
    "reg_double_2.fit(X_double_2,y_double_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "06ccb0da-7d25-4164-b955-093182224759",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_2, test_gold_2 = extract_ARG_type_feature_and_label_with_prediction(test_features, double_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1b992e1e-79cb-428d-a1f1-459ed4848fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(test_features)\n",
    "double_pred_2 = classify_data_with_rewrite(using_test_set_2,vec_double_2,reg_double_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6fae1989-fb17-4927-946f-7358c3e410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cp_output_file_2(predictions, double_pred_1, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w', encoding='utf8')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'pred1' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + str(double_pred_1[i]) + '\\t'+ predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e763f3b0-3bb7-4a45-9553-057c2597fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/GPUdoublelogreg.csv'\n",
    "write_cp_output_file_2(double_pred_2, double_pred_1, using_test_set_2, test_gold_2, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb855e-c991-49e2-bee4-8c9f03a124f6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d69b4a-ecd4-4436-9be7-e5e3ca48bb2e",
   "metadata": {},
   "source": [
    "### Sklearn - CPU - 100 it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "efc0fc2b-e20a-49be-ba8d-41ab6178e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = list(test_gold) + list(gold_labels)\n",
    "label_set = set(sorted(test_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1726bc00-e0a5-4f9f-9f85-32b5399a8743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_single_cpu = classification_report(test_gold, single_predictions, digits = 7, target_names = label_set)\n",
    "\n",
    "report_double_1_cpu = classification_report(test_gold_1, predictions_1, digits = 7, target_names = ['True', 'False'])\n",
    "report_double_2_cpu = classification_report(test_gold_2, predictions_2, digits = 7, target_names = label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1716ddc5-6c13-45b9-9d92-a708e07fd12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True  0.9286283 0.9856858 0.9563068     93404\n",
      "       False  0.6363884 0.2485132 0.3574429      9416\n",
      "\n",
      "    accuracy                      0.9181774    102820\n",
      "   macro avg  0.7825084 0.6170995 0.6568748    102820\n",
      "weighted avg  0.9018657 0.9181774 0.9014643    102820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_double_1_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abccf9cb-0372-4071-a5d2-eb66bb8baec9",
   "metadata": {},
   "source": [
    "### cuml - GPU - 50000 it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ce004968-e679-400a-ab70-2ffbe83e25f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_single_gpu = classification_report(test_gold, single_pred, digits = 7, target_names = label_set)\n",
    "\n",
    "report_double_1_gpu = classification_report(test_gold_1, double_pred_1, digits = 7, target_names = ['True', 'False'])\n",
    "report_double_2_gpu = classification_report(test_gold_2, double_pred_2, digits = 7, target_names = label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "92cd00c0-74de-48bb-abcc-e3183f8d5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    ARGM-ADV  0.4884488 0.2570932 0.3368741      1727\n",
      "  R-ARGM-TMP  0.5644788 0.2260359 0.3228086      3234\n",
      "           _  0.0000000 0.0000000 0.0000000         4\n",
      "  R-ARGM-DIR  0.6725441 0.2369122 0.3503937      1127\n",
      "    ARGM-ADJ  1.0000000 0.0270270 0.0526316        74\n",
      "    ARGM-CXN  0.7777778 0.1250000 0.2153846        56\n",
      "         C-V  0.0000000 0.0000000 0.0000000         1\n",
      "  R-ARGM-MNR  0.0000000 0.0000000 0.0000000         2\n",
      "        ARG3  0.7445255 0.4513274 0.5619835       226\n",
      "    ARGM-COM  0.6250000 0.1518219 0.2442997       494\n",
      "      R-ARG2  0.3333333 0.0434783 0.0769231        46\n",
      "        ARG5  0.0000000 0.0000000 0.0000000        13\n",
      "    ARGM-CAU  0.7500000 0.2500000 0.3750000        12\n",
      "  C-ARGM-CXN  0.4166667 0.1063830 0.1694915        47\n",
      "    ARGM-DIS  0.5757576 0.3131868 0.4056940       182\n",
      "    ARG1-DSP  0.7187500 0.6571429 0.6865672       105\n",
      "        ARG0  0.0000000 0.0000000 0.0000000        24\n",
      "  R-ARGM-LOC  0.2608696 0.0289855 0.0521739       207\n",
      "    ARGM-DIR  0.9090909 0.1449275 0.2500000        69\n",
      "      C-ARG1  0.2413793 0.0472973 0.0790960       148\n",
      "  C-ARG1-DSP  0.6967930 0.5407240 0.6089172       442\n",
      "    ARGM-EXT  0.8128342 0.7037037 0.7543424       216\n",
      "    ARGM-LVB  0.2500000 0.0227273 0.0416667        44\n",
      "        ARG1  0.0000000 0.0000000 0.0000000        75\n",
      "  C-ARGM-LOC  0.8571429 0.1739130 0.2891566        69\n",
      "    ARGM-LOC  0.6746988 0.2062615 0.3159379       543\n",
      "    ARGM-PRD  0.0000000 0.0000000 0.0000000         3\n",
      "      C-ARG0  0.0000000 0.0000000 0.0000000        52\n",
      "    ARGM-TMP  0.0000000 0.0000000 0.0000000         1\n",
      "        ARG2  1.0000000 0.1428571 0.2500000         7\n",
      "    ARGM-MNR  0.0000000 0.0000000 0.0000000         2\n",
      "    ARGM-MOD  0.0000000 0.0000000 0.0000000         5\n",
      "        ARGA  0.0000000 0.0000000 0.0000000         1\n",
      "    ARGM-PRP  0.0000000 0.0000000 0.0000000        16\n",
      "    ARGM-NEG  0.6571429 0.3432836 0.4509804        67\n",
      "  R-ARGM-ADJ  0.2727273 0.0576923 0.0952381        52\n",
      "  R-ARGM-ADV  0.0000000 0.0000000 0.0000000         1\n",
      "    ARGM-PRR  0.0000000 0.0000000 0.0000000         1\n",
      "        ARG4  0.0000000 0.0000000 0.0000000         1\n",
      "    ARGM-GOL  0.0000000 0.0000000 0.0000000         1\n",
      "      C-ARG2  0.0000000 0.0000000 0.0000000         9\n",
      "      R-ARG1  0.0000000 0.0000000 0.0000000         8\n",
      "      R-ARG0  0.0000000 0.0000000 0.0000000         2\n",
      "      C-ARG3  0.9312782 0.9861355 0.9579221     93404\n",
      "\n",
      "    accuracy                      0.9184886    102820\n",
      "   macro avg  0.3461645 0.1419072 0.1805337    102820\n",
      "weighted avg  0.8983525 0.9184886 0.9008176    102820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_single_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613d750-e960-4f5b-b67d-a2b8ef6a9eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b32ebf-0ff3-4581-b142-ab32cc4ecf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c1189-7472-4f8a-9fbe-306de24ba92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1273d-40f1-479f-aef7-2af5823317c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e5653-fece-410a-8c5a-02acc0304a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f746807-8b81-4ca6-9c8d-f2e55bdae17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f77a12-60d8-4783-a12e-578cf029e849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
