{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f9cc43-ffd7-44f7-a33f-78a33800775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086a2bf0-f8bc-4e04-a35c-f69604c66123",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = 'data/en_ewt-up-train.conllu'\n",
    "devfile = 'data/en_ewt-up-dev.conllu'\n",
    "testfile = 'data/en_ewt-up-test.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9dc1d3-3f2d-4029-b658-02c0c0f9fdb0",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39d315a-ba2b-4f12-a19a-ee08d87e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(conllfile):\n",
    "    \"\"\"\n",
    "    This function read and process the conllu file into list of sentences lists.\n",
    "    \"\"\"\n",
    "    with open(conllfile, 'r', encoding='utf8') as infile:\n",
    "        fulllist, sentlist = [],[]\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if (line != '\\n') & (line.startswith(\"#\") == False): # Not empty and not commented\n",
    "                sentlist.append(line.split())\n",
    "            if line.startswith(\"#\") == True:\n",
    "                sentlist = [i for i in sentlist if i] # Remove empty list\n",
    "                fulllist.append(sentlist)\n",
    "                sentlist = []\n",
    "                continue\n",
    "        res = [ele for ele in fulllist if ele != []] # remove empty list\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff1486c-e2be-4487-baae-83132e1d58c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainlist = read_conll(trainfile)\n",
    "devlist = read_conll(devfile)\n",
    "testlist = read_conll(testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfa092-443d-4d1c-b83d-965b94285b96",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Extract features from dataset and duplicate sentences with multiple predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c2910-aac5-49cf-a7dd-7a901f8cfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_list(conlllist):\n",
    "    \"\"\"\n",
    "    This function preprocess the lists into list of sentences list.\n",
    "    Each sentence list is a list of token lists. Each token list have 13 columns.\n",
    "    If a sentence have 0 predicates, the column (list item) 12 and 13 (list[11] and list[12]) are set as None.\n",
    "    If the sentence have multiple predicates, it will be duplicated to align the column number.\n",
    "    \"\"\"\n",
    "    sentlist = []\n",
    "    for sentence in conlllist:\n",
    "        sents = [ [] for _ in range(50) ] # Initialize a large empty list for multiple predicate sentence    \n",
    "        \n",
    "        for x in range(len(sentence)): # replace 'for components in sentence' that brings duplicate removal error\n",
    "            components = []\n",
    "            for y in range(len(sentence[x])):\n",
    "                components.append(str(sentence[x][y]))\n",
    "\n",
    "            # First 11 lines\n",
    "            for i in range(0,10):\n",
    "                try:\n",
    "                    tokendict = {\"ID\":components[0], \"form\":components[1], \"lemma\":components[2], \"upos\":components[3], \"xpos\":components[4], \"feats\":components[5], \"head\":components[6], \n",
    "                             \"deprel\":components[7], \"deps\":components[8], \"misc\":components[9], \"pred\":components[10]}\n",
    "                except IndexError: # Wrong sentence in the dataset that have no column 11\n",
    "                    tokendict['pred'] = '_'\n",
    "\n",
    "            # If sentence have no predicate: assign the values '_'\n",
    "            if len(components) <= 11: \n",
    "                tokendict['V'], tokendict['ARG'] ,tokendict['dup'] = '_','_','_'\n",
    "                sents[0].append(tokendict)\n",
    "\n",
    "            # Sentence have one or more predicate\n",
    "            if len(components) > 11: \n",
    "                dup = len(components)-11 # Times for dpulication\n",
    "                for k in range(0, dup):\n",
    "                    tokendictk = copy.deepcopy(tokendict)\n",
    "                    tokendictk['dup'] = k\n",
    "                    ARGV = components[k+11]\n",
    "                    # Following conditons change 'pred' (and ARG, V also) entry for duplicated sentence\n",
    "                    if ARGV == 'V':\n",
    "                        tokendictk['V'],tokendictk['ARG'] = 'V','_'\n",
    "                        try:\n",
    "                            tokendictk['pred'] = sentence[int(tokendictk['ID'])-1][10]\n",
    "                        except IndexError:\n",
    "                            print(sentence)\n",
    "                            continue\n",
    "                    if (ARGV != 'V') & (ARGV != '_'):\n",
    "                        tokendictk['ARG'],tokendictk['V'],tokendictk['pred'] = ARGV,'_','_'\n",
    "                    if ARGV == '_':\n",
    "                        tokendictk['V'],tokendictk['ARG'],tokendictk['pred'] = '_','_','_'\n",
    "                    sents[k].append(tokendictk)\n",
    "\n",
    "\n",
    "        res = [ele for ele in sents if ele != []] # remove empty list\n",
    "        sentlist += res\n",
    "\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed797b3-3e11-4c3c-81c4-7a4f7cac6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'I', 'I', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=1|PronType=Prs', '2', 'nsubj', '2:nsubj|9.1:nsubj|10:nsubj', '_', '_', 'ARG0', '_', '_'], ['2', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '0', 'root', '0:root', '_', 'wish.01', 'V', '_', '_'], ['3', 'all', 'all', 'DET', 'DT', '_', '2', 'iobj', '2:iobj', '_', '_', 'ARG2', '_', '_'], ['4', 'happy', 'happy', 'ADJ', 'JJ', 'Degree=Pos', '5', 'amod', '5:amod', '_', '_', '_', 'ARGM-ADJ', '_'], ['5', 'holidays', 'holiday', 'NOUN', 'NNS', 'Number=Plur', '2', 'obj', '2:obj', 'SpaceAfter=No', 'holiday.01', 'ARG1', 'V', '_'], ['6', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['7', 'and', 'and', 'CCONJ', 'CC', '_', '10', 'cc', '9.1:cc|10:cc', '_', '_', '_', '_', '_'], ['8', 'moreso', 'moreso', 'ADV', 'RB', '_', '10', 'orphan', '9.1:advmod', 'SpaceAfter=No', '_', '_', '_', '_'], ['9', ',', ',', 'PUNCT', ',', '_', '10', 'punct', '9.1:punct|10:punct', '_', '_', '_', '_', '_'], ['9.1', 'wish', 'wish', 'VERB', 'VBP', 'Mood=Ind|Tense=Pres|VerbForm=Fin', '_', '_', '2:conj:and', 'CopyOf=2'], ['10', 'peace', 'peace', 'NOUN', 'NN', 'Number=Sing', '2', 'conj', '2:conj:and|9.1:obj', '_', 'peace.01', 'ARG1', '_', 'V'], ['11', 'on', 'on', 'ADP', 'IN', '_', '12', 'case', '12:case', '_', '_', '_', '_', '_'], ['12', 'earth', 'earth', 'NOUN', 'NN', 'Number=Sing', '10', 'nmod', '10:nmod:on', 'SpaceAfter=No', '_', '_', '_', 'ARG1'], ['13', '.', '.', 'PUNCT', '.', '_', '2', 'punct', '2:punct', '_', '_', '_', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_list(trainlist)\n",
    "preprocessed_dev = preprocess_list(devlist)\n",
    "preprocessed_test = preprocess_list(testlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f412f-7a63-4a83-8cbc-85cad61e0b19",
   "metadata": {},
   "source": [
    "## Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d164bd9a-a3d2-4fa1-9ae5-8f409eea2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(preplist):\n",
    "    \"\"\"\n",
    "    This function clacluates basic statistics for the dataset.\n",
    "    Statistics: num_sent, num_token, sent_without_pred, duplicated_sent, num_pred, num_arg\n",
    "    \"\"\"\n",
    "    num_sent = len(preplist)\n",
    "    num_token, num_pred, num_arg, without_pred, duplicated_sent = 0,0,0,0,0\n",
    "    for sentence in preplist:\n",
    "        with_pred, dup_sent = False, False\n",
    "        num_token_sent, num_pred_sent, num_arg_sent = 0,0,0\n",
    "        for token in sentence:\n",
    "            if token['V'] != '_':\n",
    "                with_pred = True\n",
    "                num_pred_sent += 1\n",
    "            if (token['dup'] != 0) & (token['dup'] != '_'): dup_sent = True\n",
    "            if token['ARG'] != '_': num_arg_sent += 1\n",
    "            num_token_sent += 1\n",
    "        num_token += num_token_sent\n",
    "        num_pred += num_pred_sent\n",
    "        num_arg += num_arg_sent\n",
    "        if with_pred == False: without_pred += 1 \n",
    "        if dup_sent == True: duplicated_sent += 1\n",
    "\n",
    "    print('Sentence  Token     Predicates  Arguments  sent_without_pred  dup_sent')\n",
    "    print(num_sent,'\\t', num_token,'\\t', num_pred,'\\t', num_arg,'\\t\\t', without_pred,'\\t\\t', duplicated_sent)\n",
    "    print('PERCENTAGE    dup    without_pred    Predicate    Argument')\n",
    "    print('           ','{:.3f}'.format(100*duplicated_sent/num_sent),'\\t', '{:.3f}'.format(100*without_pred/num_sent),'\\t  ', \n",
    "          '{:.3f}'.format(100*num_pred/num_token),'\\t', '{:.3f}'.format(100*num_arg/num_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a252d040-11b3-46eb-8398-7e3b494879f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  Token     Predicates  Arguments  sent_without_pred  dup_sent\n",
      "42466 \t 1035797 \t 40496 \t 82436 \t\t 1990 \t\t 29924\n",
      "PERCENTAGE    dup    without_pred    Predicate    Argument\n",
      "            70.466 \t 4.686 \t   3.910 \t 7.959\n"
     ]
    }
   ],
   "source": [
    "get_statistics(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "43b233e4-c666-4905-9b73-c28a79c2d5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  Token     Predicates  Arguments  sent_without_pred  dup_sent\n",
      "5328 \t 103046 \t 4792 \t 9420 \t\t 539 \t\t 3252\n",
      "PERCENTAGE    dup    without_pred    Predicate    Argument\n",
      "            61.036 \t 10.116 \t   4.650 \t 9.142\n"
     ]
    }
   ],
   "source": [
    "get_statistics(preprocessed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90418fea-23c5-473a-80af-cb0d40e6a414",
   "metadata": {},
   "source": [
    "## TO BE IMPLEMENTED: GET OTHER FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26028649-a33d-4317-94cb-6110ed3b5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(preplist):\n",
    "    \"\"\"\n",
    "    This function creates extra features by dependency parsing, using a preprocessed list.\n",
    "    \"\"\"\n",
    "    sent_with_feature = []\n",
    "    \n",
    "    for sentence in preplist:\n",
    "        \n",
    "        # Extract sentence text\n",
    "        sentence_text = []\n",
    "        for dict in sentence:\n",
    "            sentence_text += dict['form']\n",
    "            \n",
    "        # IMPLEMENT HERE: Extract sentence features\n",
    "        #feat1, feat2, ... = extract_parsing_features(sentence_text)\n",
    "\n",
    "        # Add features back to dict\n",
    "        newsent = []\n",
    "        for dict in sentence:\n",
    "            newdict = copy.deepcopy(dict) # Avoid changing the original file\n",
    "            #newdict['feat1'], newdict['feat2'], ... = feat1, feat2\n",
    "            newsent.append(newdict)\n",
    "\n",
    "        sent_with_feature.append(newsent)\n",
    "\n",
    "    return sent_with_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca47588-3f9d-433d-b393-ce49762716e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parsing_features():\n",
    "    \"\"\" TO BE IMPLEMENTED \u0000 \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63642da-e721-4032-bf7b-d7a66972117a",
   "metadata": {},
   "source": [
    "## Single classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a8678-9ecb-404c-bee7-f7bd6d655860",
   "metadata": {},
   "source": [
    "### Extract training features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21423387-fc45-419d-b804-24435c5893d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from extracted feature list of dicts.\n",
    "    It will flattern list of sentences into list of tokens.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "821ead1d-3d2b-4f92-8921-814f0407e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features, gold_labels = extract_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa7317-724c-444f-9054-fe94cc4c40fa",
   "metadata": {},
   "source": [
    "### Create single logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac80732a-cb99-48ae-a46a-f9b45ea8183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def create_log_classifier(train_features, train_targets, max_iter):\n",
    "    logreg = LogisticRegression(max_iter=max_iter)\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    model = logreg.fit(features_vectorized, train_targets) \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdce8e0-8caf-49bd-8e5c-4289172a2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_single, vec_single = create_log_classifier(training_features, gold_labels, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc0d5f-2948-4e83-b81c-d14e02683931",
   "metadata": {},
   "source": [
    "### Predict with single logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b5cb68-1629-4908-b000-9ebb3c832268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, features):  \n",
    "    features = vec.transform(features)\n",
    "    predictions = model.predict(features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bf2eed-63c3-49d9-9571-ca461620bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test)\n",
    "single_predictions = classify_data(model_single, vec_single, using_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ea878-cd8c-4e8c-8eca-3ca2bc19362f",
   "metadata": {},
   "source": [
    "### Write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d197c7-42ad-4814-b17b-51692ef952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_file(predictions, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c471942d-587a-4f50-b62f-e4fbd9905245",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/singlelogreg.csv'\n",
    "write_output_file(single_predictions, using_test_set, test_gold, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7bfa1-d6a7-45e5-942c-eaf03f833ee6",
   "metadata": {},
   "source": [
    "## Double classifier\n",
    "First classify is_ARG, then classify ARG_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02f4bdc-f814-4ea5-b27c-a7d5d14cda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_is_ARG_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract features and label from preprocessed list\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        data.append(newdict)\n",
    "        \n",
    "        if dict['ARG'] != '_':\n",
    "            targets.append(True)\n",
    "        else:\n",
    "            targets.append(False)\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f9251-ad95-4f2a-b380-d3d5d4140a65",
   "metadata": {},
   "source": [
    "**Here, we use less feature for step 1 to reduce overfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02139466-c1ea-4aa5-9ac5-97a81b30f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducing_features(inputfeature):\n",
    "    \"\"\"\n",
    "    This function reduce the amount of feature used. Input is the ready-to-use feature dict.\n",
    "    \"\"\"\n",
    "    newfeature = copy.deepcopy(inputfeature)\n",
    "    for newdicts in newfeature:\n",
    "        del newdicts['ID'], newdicts['lemma'], newdicts['feats'], newdicts['dup'], newdicts['misc'], newdicts['deprel']\n",
    "\n",
    "    return newfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95d6f97b-efd1-4cee-b453-388bb2acbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step1, gold_labels_step1 = extract_is_ARG_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6321564b-ed74-4f71-a9e9-8ae87b9c39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_training_features_step1 = reducing_features(training_features_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cb1b009-4f8a-47b3-a82c-0cd875db9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_double_1, vec_double_1 = create_log_classifier(reduced_training_features_step1, gold_labels_step1, 100)\n",
    "\n",
    "using_test_set_1, test_gold_1 = extract_is_ARG_feature_and_label(preprocessed_test)\n",
    "predictions_1 = classify_data(model_double_1, vec_double_1, using_test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9c606af-60bd-4cd2-937c-146e95b44fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ARG_type_feature_and_label(preplist):\n",
    "    \"\"\"\n",
    "    This function extract ARG_type feature from the training set.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    \n",
    "    for dict in flatlist:\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        if dict['ARG'] != '_':\n",
    "            newdict['is_ARG'] = 'True'\n",
    "        else:\n",
    "            newdict['is_ARG'] = 'False'\n",
    "        \n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba39c51c-cafc-45d2-b319-834158ebeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step2, gold_labels_step2 = extract_ARG_type_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56e1a8a3-a7de-4519-813b-cd3abf2895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ARG_type_feature_and_label_with_prediction(preplist, predictions_1):\n",
    "    \"\"\"\n",
    "    This function add result from the first classifier to the feature list for the test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    targets = []\n",
    "    flatlist = [x for xs in preplist for x in xs]\n",
    "    \n",
    "    for dict, predictions in zip(flatlist, predictions_1):\n",
    "        newdict = copy.deepcopy(dict)\n",
    "        del newdict['ARG'] # Remove gold\n",
    "        newdict['is_ARG'] = str(predictions)\n",
    "        \n",
    "        data.append(newdict)\n",
    "        targets.append(dict['ARG'])\n",
    "\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9999d3ee-6225-4fe4-b4b5-ab61098b54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arimo/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_double_2, vec_double_2 = create_log_classifier(training_features_step2, gold_labels_step2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7a6c984-3698-4f69-acf1-b2a32133fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_2, test_gold_2 = extract_ARG_type_feature_and_label_with_prediction(preprocessed_test, predictions_1)\n",
    "\n",
    "predictions_2 = classify_data(model_double_2, vec_double_2, using_test_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76f23d02-b5ed-4945-84a0-a6cff4e1993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/doublelogreg.csv'\n",
    "write_output_file(predictions_2, using_test_set_2, test_gold_2, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3a1b7-14c0-44ed-8145-50660f5ff510",
   "metadata": {},
   "source": [
    "## GPU implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a024ecdb-cbd8-48a6-9f2c-47f16c24c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103f4734-a5e8-43f3-8a3b-fe42a7f8afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dict_vectorize(train_features):\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    return vec, features_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "592f885b-0ee6-460d-b41c-518dc328f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings_dict(inputlist):\n",
    "    category_list = copy.deepcopy(inputlist)\n",
    "    category_list.append(None)\n",
    "    map_dict = dict(zip(set(category_list), range(len(set(category_list)))))\n",
    "    map_dict_reverse = {v: k for k, v in map_dict.items()}\n",
    "    return map_dict, map_dict_reverse\n",
    "\n",
    "def numerical_mapping(category_list, list_dict):\n",
    "    numerical_list = [list_dict[line] for line in category_list]\n",
    "    return numerical_list\n",
    "\n",
    "mydict, mydict_rev = get_mappings_dict(gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97f45222-cc0b-4f33-97e3-d83b45d2344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_feature_and_gold(feat_vec, gold_labels=gold_labels, dict=mydict):\n",
    "    # This function convert array to cuml required cp_array type\n",
    "    cpfeature = cp.sparse.csr_matrix(feat_vec)\n",
    "    cpgold = [dict[line] for line in gold_labels]\n",
    "    cpgold = cp.array(cpgold, dtype=cp.int64)\n",
    "    return cpfeature, cpgold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d07fd814-2406-4ab1-8643-6dee8c98cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data_with_rewrite(using_test_set, vec, model, dict=mydict_rev):  \n",
    "    features = vec.transform(using_test_set)\n",
    "    predictions = model.predict(features)\n",
    "    rw_predictions = [dict[line] for line in predictions]\n",
    "    return rw_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935008c-19e9-4558-872b-df61df336e63",
   "metadata": {},
   "source": [
    "### Single classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "291f2fae-eb84-4e2b-94d9-d55c914eb069",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_single, feat_vec_single = dict_vectorize(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df689fd8-ffe8-409b-9f5d-9f1f394be76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_single, y_single = cp_feature_and_gold(feat_vec_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50d16b49-e5f4-4c53-9b20-c8adfb5e31b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_single = LogisticRegression(max_iter=50000,class_weight='balanced')\n",
    "reg_single.fit(X_single,y_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad92825a-2914-40ba-89ec-93b1eb92f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test)\n",
    "single_pred = classify_data_with_rewrite(using_test_set,vec_single,reg_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da5ab6ce-d9da-45c5-9422-7ae342056074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cp_output_file(predictions, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w', encoding='utf8')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c37f4b9-ea66-4c40-802b-24ac4cacdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/GPUsinglelogreg.csv'\n",
    "write_cp_output_file(single_pred, using_test_set, test_gold, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073f2d3-c490-426a-b03c-09d9c0673ffc",
   "metadata": {},
   "source": [
    "### Double classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "900201a6-a65c-4fb6-a332-f5f208641903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec_double_1, feat_vec_double_1 = dict_vectorize(training_features_step1)\n",
    "vec_double_1, feat_vec_double_1 = dict_vectorize(reduced_training_features_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "000bb280-c393-46af-a88c-da189c463b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_double_1 = cp.sparse.csr_matrix(feat_vec_double_1)\n",
    "y_double_1 = cp.array(gold_labels_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2a48aa9-f6f3-4d28-b410-436d70b93292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_double_1 = LogisticRegression(max_iter=5000)\n",
    "reg_double_1.fit(X_double_1,y_double_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eeef5d72-0e0c-455d-ba1d-0b10ff7da8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_1, test_gold_1 = extract_is_ARG_feature_and_label(preprocessed_test)\n",
    "using_test_set_1_vec = vec_double_1.transform(using_test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a43e693-fc43-47ea-b72c-8dd5196d2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_pred_1 = reg_double_1.predict(using_test_set_1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba487d3-3e2a-4255-8900-536a191bebab",
   "metadata": {},
   "source": [
    "### Quick overfit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2f129c5-178f-4d7a-b01e-1a57b8ccd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report_double_1 = classification_report(test_gold_1, double_pred_1, digits = 7, target_names = ['True', 'False'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb7ab129-cac7-4e23-b2c2-215c1ae9ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True  0.9107113 0.9970201 0.9519133     93626\n",
      "       False  0.4899452 0.0284501 0.0537775      9420\n",
      "\n",
      "    accuracy                      0.9084778    103046\n",
      "   macro avg  0.7003282 0.5127351 0.5028454    103046\n",
      "weighted avg  0.8722468 0.9084778 0.8698098    103046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_double_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6de4d-767e-4b6a-96f4-dcef60788f59",
   "metadata": {},
   "source": [
    "#### Second classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "932f2cc0-cffa-43e4-865e-09a8225992d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_step2, gold_labels_step2 = extract_ARG_type_feature_and_label(preprocessed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28916dcf-8c74-4853-9fc4-20a0ceaae721",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_double_2, feat_vec_double_2 = dict_vectorize(training_features_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3ace737-05db-44aa-9e9d-1eeccbdb5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_double_2, y_double_2 = cp_feature_and_gold(feat_vec_double_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fa8c01b-90be-4ec7-9439-fff021b6d0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_double_2 = LogisticRegression(max_iter=50000,class_weight='balanced')\n",
    "reg_double_2.fit(X_double_2,y_double_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06ccb0da-7d25-4164-b955-093182224759",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set_2, test_gold_2 = extract_ARG_type_feature_and_label_with_prediction(preprocessed_test, double_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b992e1e-79cb-428d-a1f1-459ed4848fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_test_set, test_gold = extract_feature_and_label(preprocessed_test)\n",
    "double_pred_2 = classify_data_with_rewrite(using_test_set_2,vec_double_2,reg_double_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fae1989-fb17-4927-946f-7358c3e410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cp_output_file_2(predictions, double_pred_1, training_features, gold_labels, outputfile):\n",
    "    outfile = open(outputfile, 'w', encoding='utf8')\n",
    "    # add headings\n",
    "    outfile.write('word' + '\\t' + 'gold' + '\\t' + 'pred1' + '\\t' + 'predict' + '\\n')\n",
    "    for i in range(len(predictions)):\n",
    "        outfile.write(training_features[i]['form'] + '\\t' + gold_labels[i] + '\\t' + str(double_pred_1[i]) + '\\t'+ predictions[i] + '\\n')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e763f3b0-3bb7-4a45-9553-057c2597fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = 'output/GPUdoublelogreg.csv'\n",
    "write_cp_output_file_2(double_pred_2, double_pred_1, using_test_set_2, test_gold_2, outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb855e-c991-49e2-bee4-8c9f03a124f6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d69b4a-ecd4-4436-9be7-e5e3ca48bb2e",
   "metadata": {},
   "source": [
    "### Sklearn - CPU - 100 it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efc0fc2b-e20a-49be-ba8d-41ab6178e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = list(test_gold) + list(gold_labels)\n",
    "label_set = set(sorted(test_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726bc00-e0a5-4f9f-9f85-32b5399a8743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report_single_cpu = classification_report(test_gold, single_predictions, digits = 7, target_names = label_set)\n",
    "\n",
    "report_double_1_cpu = classification_report(test_gold_1, predictions_1, digits = 7, target_names = ['True', 'False'])\n",
    "report_double_2_cpu = classification_report(test_gold_2, predictions_2, digits = 7, target_names = label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1716ddc5-6c13-45b9-9d92-a708e07fd12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True  0.9101635 0.9978104 0.9519738     93626\n",
      "       False  0.4925743 0.0211253 0.0405130      9420\n",
      "\n",
      "    accuracy                      0.9085263    103046\n",
      "   macro avg  0.7013689 0.5094679 0.4962434    103046\n",
      "weighted avg  0.8719894 0.9085263 0.8686522    103046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_double_1_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abccf9cb-0372-4071-a5d2-eb66bb8baec9",
   "metadata": {},
   "source": [
    "### cuml - GPU - 50000 it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce004968-e679-400a-ab70-2ffbe83e25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_single_gpu = classification_report(test_gold, single_pred, digits = 7, target_names = label_set)\n",
    "\n",
    "report_double_1_gpu = classification_report(test_gold_1, double_pred_1, digits = 7, target_names = ['True', 'False'])\n",
    "report_double_2_gpu = classification_report(test_gold_2, double_pred_2, digits = 7, target_names = label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92cd00c0-74de-48bb-abcc-e3183f8d5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C-V  0.3523810 0.0214120 0.0403710      1728\n",
      "           _  0.4212329 0.0380216 0.0697477      3235\n",
      "  C-ARGM-CXN  0.0000000 0.0000000 0.0000000         4\n",
      "    ARGM-LOC  0.3473684 0.0292813 0.0540098      1127\n",
      "    ARGM-CXN  1.0000000 0.0135135 0.0266667        74\n",
      "  C-ARG1-DSP  0.0000000 0.0000000 0.0000000        56\n",
      "  R-ARGM-ADJ  0.0000000 0.0000000 0.0000000         1\n",
      "  C-ARGM-LOC  0.0000000 0.0000000 0.0000000         2\n",
      "    ARGM-TMP  0.4500000 0.0398230 0.0731707       226\n",
      "        ARG4  0.2727273 0.0120968 0.0231660       496\n",
      "        ARG5  0.0000000 0.0000000 0.0000000        46\n",
      "      R-ARG0  0.0000000 0.0000000 0.0000000        13\n",
      "        ARG1  0.0000000 0.0000000 0.0000000        12\n",
      "    ARGM-PRD  0.0000000 0.0000000 0.0000000        47\n",
      "    ARGM-ADJ  0.5625000 0.2472527 0.3435115       182\n",
      "    ARGM-ADV  1.0000000 0.0666667 0.1250000       105\n",
      "  R-ARGM-DIR  0.0000000 0.0000000 0.0000000        24\n",
      "    ARGM-LVB  0.2222222 0.0193237 0.0355556       207\n",
      "        ARGA  0.0000000 0.0000000 0.0000000        69\n",
      "    ARGM-DIR  0.0000000 0.0000000 0.0000000       148\n",
      "      C-ARG0  0.3750000 0.0135747 0.0262009       442\n",
      "    ARGM-DIS  0.0000000 0.0000000 0.0000000       216\n",
      "    ARGM-CAU  0.0000000 0.0000000 0.0000000        44\n",
      "    ARGM-NEG  1.0000000 0.0133333 0.0263158        75\n",
      "    ARGM-GOL  0.0000000 0.0000000 0.0000000        69\n",
      "    ARGM-MNR  0.4047619 0.0313076 0.0581197       543\n",
      "    ARGM-EXT  0.0000000 0.0000000 0.0000000         3\n",
      "      C-ARG2  0.0000000 0.0000000 0.0000000        52\n",
      "    ARGM-PRR  0.0000000 0.0000000 0.0000000         1\n",
      "  R-ARGM-TMP  0.0000000 0.0000000 0.0000000         7\n",
      "    ARGM-MOD  0.0000000 0.0000000 0.0000000         2\n",
      "        ARG2  0.0000000 0.0000000 0.0000000         5\n",
      "        ARG3  0.0000000 0.0000000 0.0000000         1\n",
      "      C-ARG1  0.0000000 0.0000000 0.0000000        16\n",
      "  R-ARGM-ADV  0.0000000 0.0000000 0.0000000        67\n",
      "      C-ARG3  0.0000000 0.0000000 0.0000000        52\n",
      "  R-ARGM-LOC  0.0000000 0.0000000 0.0000000         1\n",
      "    ARGM-COM  0.0000000 0.0000000 0.0000000         1\n",
      "      R-ARG1  0.0000000 0.0000000 0.0000000         1\n",
      "  R-ARGM-MNR  0.0000000 0.0000000 0.0000000         1\n",
      "    ARG1-DSP  0.0000000 0.0000000 0.0000000         9\n",
      "      R-ARG2  0.0000000 0.0000000 0.0000000         8\n",
      "    ARGM-PRP  0.0000000 0.0000000 0.0000000         2\n",
      "        ARG0  0.9114859 0.9962617 0.9519902     93626\n",
      "\n",
      "    accuracy                      0.9079925    103046\n",
      "   macro avg  0.1663564 0.0350425 0.0421324    103046\n",
      "weighted avg  0.8610402 0.9079925 0.8699553    103046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_single_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613d750-e960-4f5b-b67d-a2b8ef6a9eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
